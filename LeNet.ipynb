{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LeNet.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5Ij9DKNFO0Yk","colab_type":"code","outputId":"30af43c7-380d-4454-9a41-0cf36c903930","executionInfo":{"status":"ok","timestamp":1560229521916,"user_tz":-330,"elapsed":9379,"user":{"displayName":"ANUSHA SHENOY (01FB15EEC044)PESU ECE STUDENT","photoUrl":"","userId":"17943164030553000523"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":[" ! git clone https://Nusha:inceptionv3@gitlab.com/Nusha/abcdefghi\n","      "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'abcdefghi'...\n","warning: redirecting to https://gitlab.com/Nusha/abcdefghi.git/\n","remote: Enumerating objects: 877, done.\u001b[K\n","remote: Counting objects: 100% (877/877), done.\u001b[K\n","remote: Compressing objects: 100% (876/876), done.\u001b[K\n","remote: Total 877 (delta 0), reused 877 (delta 0)\u001b[K\n","Receiving objects: 100% (877/877), 10.28 MiB | 24.71 MiB/s, done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cN-2d5iTR75o","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-h6SHQDiTKNQ","colab_type":"code","outputId":"7dd8a2f9-f90d-45c2-fe7b-c0965cc79921","executionInfo":{"status":"ok","timestamp":1560229583807,"user_tz":-330,"elapsed":866,"user":{"displayName":"ANUSHA SHENOY (01FB15EEC044)PESU ECE STUDENT","photoUrl":"","userId":"17943164030553000523"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from torch.utils.data.sampler import SubsetRandomSampler\n","\n","data_dir = 'abcdefghi/data_die'\n","\n","def load_split_train_test(datadir, valid_size = .2):\n","    train_transforms = transforms.Compose([transforms.Resize(32), transforms.ToTensor(),])\n","    test_transforms = transforms.Compose([transforms.Resize(32), transforms.ToTensor(),])\n","    valid_transforms = transforms.Compose([transforms.Resize(32), transforms.ToTensor(),])\n","    \n","    train_data = datasets.ImageFolder(datadir, transform=train_transforms)\n","    test_data = datasets.ImageFolder(datadir, transform=test_transforms)\n","    valid_data = datasets.ImageFolder(datadir, transform=train_transforms) \n","    \n","    num_train = len(train_data)\n","    indices = list(range(num_train))\n","    split = int(np.floor(valid_size * num_train))\n","    np.random.shuffle(indices)\n","    \n","    train_idx, test_idx = indices[split:], indices[:split]\n","    \n","    num_train = len(train_idx)\n","    indices_train = list(range(num_train))\n","    np.random.shuffle(indices_train)\n","    split_tv = int(np.floor(valid_size * num_train))\n","    train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n","    \n","    train_sampler = SubsetRandomSampler(train_idx)\n","    test_sampler = SubsetRandomSampler(test_idx)\n","    valid_sampler = SubsetRandomSampler(valid_idx)\n","    \n","    \n","    trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=64)\n","    testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=64)\n","    validloader = torch.utils.data.DataLoader(valid_data, sampler=valid_sampler, batch_size=64)\n","    \n","    return trainloader, testloader, validloader\n","\n","trainloader, testloader, validloader = load_split_train_test(data_dir, .2)\n","print(trainloader.dataset.classes)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["['autistic', 'neurotypical']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KzFQV9yN7jpQ","colab_type":"code","colab":{}},"source":["class LeNet(nn.Module):\n","    def __init__(self):\n","        super(LeNet, self).__init__()\n","\n","        # input channel = 3, output channel = 6, kernel_size = 5\n","        # input size = (32, 32), output size = (28, 28)\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        # input channel = 6, output channel = 16, kernel_size = 5\n","        # input size = (14, 14), output size = (10, 10)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        # input dim = 16*5*5, output dim = 120\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        # input dim = 120, output dim = 84\n","        self.fc2 = nn.Linear(120, 84)\n","        # input dim = 84, output dim = 10\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        # pool size = 2\n","        # input size = (28, 28), output size = (14, 14), output channel = 6\n","        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n","        # pool size = 2\n","        # input size = (10, 10), output size = (5, 5), output channel = 16\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        # flatten as one dimension\n","        x = x.view(x.shape[0], -1)\n","        # input dim = 16*5*5, output dim = 120\n","        x = F.relu(self.fc1(x))\n","        # input dim = 120, output dim = 84\n","        x = F.relu(self.fc2(x))\n","        # input dim = 84, output dim = 10\n","        x = self.fc3(x)\n","        return x\n","    \n","net = LeNet()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZ83aUNd7zPu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"a7b0f4df-f14f-49d0-ff68-60ec6c5cda5d","executionInfo":{"status":"ok","timestamp":1560229804973,"user_tz":-330,"elapsed":8375,"user":{"displayName":"ANUSHA SHENOY (01FB15EEC044)PESU ECE STUDENT","photoUrl":"","userId":"17943164030553000523"}}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = LeNet().to(device)\n","print(model)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["LeNet(\n","  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (fc1): Linear(in_features=400, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z-XH69Gt8YP0","colab_type":"code","colab":{}},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ExL2MPqE8c8a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":17289},"outputId":"3b14dcb3-e02f-4230-9dbb-dc6728ccb9b7","executionInfo":{"status":"ok","timestamp":1560230391837,"user_tz":-330,"elapsed":545805,"user":{"displayName":"ANUSHA SHENOY (01FB15EEC044)PESU ECE STUDENT","photoUrl":"","userId":"17943164030553000523"}}},"source":["# number of epochs to train the model\n","n_epochs = 1000 # you may increase this number to train a final model\n","\n","classes= [0,1]\n","\n","valid_loss_min = np.Inf # track change in validation loss\n","\n","for epoch in range(1, n_epochs+1):\n","\n","    # keep track of training and validation loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    \n","    ###################\n","    # train the model #\n","    ###################\n","    model.train()\n","    for data, target in trainloader:\n","        # move tensors to GPU if CUDA is available\n","        train_on_gpu = torch.cuda.is_available()\n","        if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        # clear the gradients of all optimized variables\n","        optimizer.zero_grad()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # backward pass: compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # perform a single optimization step (parameter update)\n","        optimizer.step()\n","        # update training loss\n","        train_loss += loss.item()*data.size(0)\n","        \n","    ######################    \n","    # validate the model #\n","    ######################\n","    model.eval()\n","    for data, target in validloader:\n","        # move tensors to GPU if CUDA is available\n","        if train_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # update average validation loss \n","        valid_loss += loss.item()*data.size(0)\n","    \n","    # calculate average losses\n","    train_loss = train_loss/len(trainloader.dataset)\n","    valid_loss = valid_loss/len(validloader.dataset)\n","        \n","    # print training/validation statistics \n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","        epoch, train_loss, valid_loss))\n","    \n","    # save model if validation loss has decreased\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss))\n","        torch.save(model.state_dict(), 'model_cifar.pt')\n","        valid_loss_min = valid_loss"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch: 1 \tTraining Loss: 1.610936 \tValidation Loss: 0.185701\n","Validation loss decreased (inf --> 0.185701).  Saving model ...\n","Epoch: 2 \tTraining Loss: 0.626433 \tValidation Loss: 0.122045\n","Validation loss decreased (0.185701 --> 0.122045).  Saving model ...\n","Epoch: 3 \tTraining Loss: 0.572149 \tValidation Loss: 0.112589\n","Validation loss decreased (0.122045 --> 0.112589).  Saving model ...\n","Epoch: 4 \tTraining Loss: 0.558776 \tValidation Loss: 0.110655\n","Validation loss decreased (0.112589 --> 0.110655).  Saving model ...\n","Epoch: 5 \tTraining Loss: 0.573195 \tValidation Loss: 0.103540\n","Validation loss decreased (0.110655 --> 0.103540).  Saving model ...\n","Epoch: 6 \tTraining Loss: 0.588323 \tValidation Loss: 0.110705\n","Epoch: 7 \tTraining Loss: 0.562263 \tValidation Loss: 0.108125\n","Epoch: 8 \tTraining Loss: 0.559109 \tValidation Loss: 0.105213\n","Epoch: 9 \tTraining Loss: 0.564759 \tValidation Loss: 0.106903\n","Epoch: 10 \tTraining Loss: 0.559337 \tValidation Loss: 0.120202\n","Epoch: 11 \tTraining Loss: 0.563358 \tValidation Loss: 0.114609\n","Epoch: 12 \tTraining Loss: 0.565723 \tValidation Loss: 0.104860\n","Epoch: 13 \tTraining Loss: 0.557228 \tValidation Loss: 0.117224\n","Epoch: 14 \tTraining Loss: 0.562916 \tValidation Loss: 0.108161\n","Epoch: 15 \tTraining Loss: 0.557197 \tValidation Loss: 0.110957\n","Epoch: 16 \tTraining Loss: 0.556246 \tValidation Loss: 0.109912\n","Epoch: 17 \tTraining Loss: 0.557380 \tValidation Loss: 0.108666\n","Epoch: 18 \tTraining Loss: 0.555818 \tValidation Loss: 0.113949\n","Epoch: 19 \tTraining Loss: 0.558899 \tValidation Loss: 0.111328\n","Epoch: 20 \tTraining Loss: 0.557315 \tValidation Loss: 0.109616\n","Epoch: 21 \tTraining Loss: 0.556887 \tValidation Loss: 0.111952\n","Epoch: 22 \tTraining Loss: 0.555969 \tValidation Loss: 0.110042\n","Epoch: 23 \tTraining Loss: 0.556445 \tValidation Loss: 0.107468\n","Epoch: 24 \tTraining Loss: 0.556910 \tValidation Loss: 0.116184\n","Epoch: 25 \tTraining Loss: 0.563740 \tValidation Loss: 0.109330\n","Epoch: 26 \tTraining Loss: 0.565489 \tValidation Loss: 0.109633\n","Epoch: 27 \tTraining Loss: 0.560510 \tValidation Loss: 0.111183\n","Epoch: 28 \tTraining Loss: 0.557293 \tValidation Loss: 0.110252\n","Epoch: 29 \tTraining Loss: 0.557261 \tValidation Loss: 0.108720\n","Epoch: 30 \tTraining Loss: 0.555858 \tValidation Loss: 0.111114\n","Epoch: 31 \tTraining Loss: 0.557219 \tValidation Loss: 0.107927\n","Epoch: 32 \tTraining Loss: 0.560689 \tValidation Loss: 0.116179\n","Epoch: 33 \tTraining Loss: 0.560789 \tValidation Loss: 0.108637\n","Epoch: 34 \tTraining Loss: 0.557948 \tValidation Loss: 0.112547\n","Epoch: 35 \tTraining Loss: 0.557065 \tValidation Loss: 0.113197\n","Epoch: 36 \tTraining Loss: 0.555198 \tValidation Loss: 0.109431\n","Epoch: 37 \tTraining Loss: 0.556919 \tValidation Loss: 0.114544\n","Epoch: 38 \tTraining Loss: 0.558745 \tValidation Loss: 0.107373\n","Epoch: 39 \tTraining Loss: 0.556439 \tValidation Loss: 0.112633\n","Epoch: 40 \tTraining Loss: 0.557942 \tValidation Loss: 0.108964\n","Epoch: 41 \tTraining Loss: 0.558912 \tValidation Loss: 0.112393\n","Epoch: 42 \tTraining Loss: 0.556886 \tValidation Loss: 0.106973\n","Epoch: 43 \tTraining Loss: 0.558138 \tValidation Loss: 0.111301\n","Epoch: 44 \tTraining Loss: 0.557413 \tValidation Loss: 0.109940\n","Epoch: 45 \tTraining Loss: 0.557139 \tValidation Loss: 0.110566\n","Epoch: 46 \tTraining Loss: 0.558271 \tValidation Loss: 0.113512\n","Epoch: 47 \tTraining Loss: 0.557281 \tValidation Loss: 0.108148\n","Epoch: 48 \tTraining Loss: 0.557321 \tValidation Loss: 0.116075\n","Epoch: 49 \tTraining Loss: 0.557704 \tValidation Loss: 0.110468\n","Epoch: 50 \tTraining Loss: 0.556703 \tValidation Loss: 0.110720\n","Epoch: 51 \tTraining Loss: 0.558211 \tValidation Loss: 0.109503\n","Epoch: 52 \tTraining Loss: 0.559072 \tValidation Loss: 0.111546\n","Epoch: 53 \tTraining Loss: 0.556108 \tValidation Loss: 0.110393\n","Epoch: 54 \tTraining Loss: 0.557863 \tValidation Loss: 0.113579\n","Epoch: 55 \tTraining Loss: 0.558570 \tValidation Loss: 0.110427\n","Epoch: 56 \tTraining Loss: 0.555551 \tValidation Loss: 0.109692\n","Epoch: 57 \tTraining Loss: 0.555779 \tValidation Loss: 0.111186\n","Epoch: 58 \tTraining Loss: 0.556546 \tValidation Loss: 0.112585\n","Epoch: 59 \tTraining Loss: 0.555448 \tValidation Loss: 0.108158\n","Epoch: 60 \tTraining Loss: 0.557131 \tValidation Loss: 0.112228\n","Epoch: 61 \tTraining Loss: 0.556196 \tValidation Loss: 0.108531\n","Epoch: 62 \tTraining Loss: 0.556530 \tValidation Loss: 0.113374\n","Epoch: 63 \tTraining Loss: 0.556160 \tValidation Loss: 0.110893\n","Epoch: 64 \tTraining Loss: 0.555688 \tValidation Loss: 0.111864\n","Epoch: 65 \tTraining Loss: 0.556693 \tValidation Loss: 0.107558\n","Epoch: 66 \tTraining Loss: 0.556987 \tValidation Loss: 0.111894\n","Epoch: 67 \tTraining Loss: 0.557777 \tValidation Loss: 0.107378\n","Epoch: 68 \tTraining Loss: 0.556156 \tValidation Loss: 0.112512\n","Epoch: 69 \tTraining Loss: 0.556260 \tValidation Loss: 0.108371\n","Epoch: 70 \tTraining Loss: 0.556408 \tValidation Loss: 0.112541\n","Epoch: 71 \tTraining Loss: 0.558319 \tValidation Loss: 0.109102\n","Epoch: 72 \tTraining Loss: 0.556096 \tValidation Loss: 0.111873\n","Epoch: 73 \tTraining Loss: 0.556910 \tValidation Loss: 0.107640\n","Epoch: 74 \tTraining Loss: 0.556756 \tValidation Loss: 0.113145\n","Epoch: 75 \tTraining Loss: 0.557101 \tValidation Loss: 0.109727\n","Epoch: 76 \tTraining Loss: 0.555443 \tValidation Loss: 0.109792\n","Epoch: 77 \tTraining Loss: 0.555273 \tValidation Loss: 0.112630\n","Epoch: 78 \tTraining Loss: 0.556984 \tValidation Loss: 0.108046\n","Epoch: 79 \tTraining Loss: 0.558299 \tValidation Loss: 0.110967\n","Epoch: 80 \tTraining Loss: 0.556093 \tValidation Loss: 0.109973\n","Epoch: 81 \tTraining Loss: 0.555699 \tValidation Loss: 0.109260\n","Epoch: 82 \tTraining Loss: 0.555786 \tValidation Loss: 0.110384\n","Epoch: 83 \tTraining Loss: 0.555182 \tValidation Loss: 0.112117\n","Epoch: 84 \tTraining Loss: 0.555328 \tValidation Loss: 0.108858\n","Epoch: 85 \tTraining Loss: 0.556826 \tValidation Loss: 0.110924\n","Epoch: 86 \tTraining Loss: 0.556062 \tValidation Loss: 0.111792\n","Epoch: 87 \tTraining Loss: 0.557028 \tValidation Loss: 0.110294\n","Epoch: 88 \tTraining Loss: 0.556696 \tValidation Loss: 0.112975\n","Epoch: 89 \tTraining Loss: 0.556156 \tValidation Loss: 0.110743\n","Epoch: 90 \tTraining Loss: 0.557853 \tValidation Loss: 0.110228\n","Epoch: 91 \tTraining Loss: 0.557697 \tValidation Loss: 0.113235\n","Epoch: 92 \tTraining Loss: 0.558405 \tValidation Loss: 0.108464\n","Epoch: 93 \tTraining Loss: 0.560339 \tValidation Loss: 0.110843\n","Epoch: 94 \tTraining Loss: 0.556356 \tValidation Loss: 0.111790\n","Epoch: 95 \tTraining Loss: 0.555343 \tValidation Loss: 0.110347\n","Epoch: 96 \tTraining Loss: 0.556385 \tValidation Loss: 0.110952\n","Epoch: 97 \tTraining Loss: 0.556119 \tValidation Loss: 0.109365\n","Epoch: 98 \tTraining Loss: 0.556757 \tValidation Loss: 0.112131\n","Epoch: 99 \tTraining Loss: 0.555796 \tValidation Loss: 0.112006\n","Epoch: 100 \tTraining Loss: 0.555413 \tValidation Loss: 0.108677\n","Epoch: 101 \tTraining Loss: 0.558514 \tValidation Loss: 0.109231\n","Epoch: 102 \tTraining Loss: 0.559802 \tValidation Loss: 0.108485\n","Epoch: 103 \tTraining Loss: 0.561807 \tValidation Loss: 0.113963\n","Epoch: 104 \tTraining Loss: 0.557835 \tValidation Loss: 0.107784\n","Epoch: 105 \tTraining Loss: 0.558013 \tValidation Loss: 0.112658\n","Epoch: 106 \tTraining Loss: 0.556131 \tValidation Loss: 0.109944\n","Epoch: 107 \tTraining Loss: 0.555587 \tValidation Loss: 0.111820\n","Epoch: 108 \tTraining Loss: 0.556080 \tValidation Loss: 0.110738\n","Epoch: 109 \tTraining Loss: 0.556001 \tValidation Loss: 0.110955\n","Epoch: 110 \tTraining Loss: 0.555921 \tValidation Loss: 0.109951\n","Epoch: 111 \tTraining Loss: 0.558440 \tValidation Loss: 0.114447\n","Epoch: 112 \tTraining Loss: 0.558664 \tValidation Loss: 0.110572\n","Epoch: 113 \tTraining Loss: 0.557358 \tValidation Loss: 0.110071\n","Epoch: 114 \tTraining Loss: 0.555262 \tValidation Loss: 0.112954\n","Epoch: 115 \tTraining Loss: 0.554673 \tValidation Loss: 0.108632\n","Epoch: 116 \tTraining Loss: 0.556890 \tValidation Loss: 0.109545\n","Epoch: 117 \tTraining Loss: 0.556547 \tValidation Loss: 0.112702\n","Epoch: 118 \tTraining Loss: 0.556155 \tValidation Loss: 0.110233\n","Epoch: 119 \tTraining Loss: 0.555840 \tValidation Loss: 0.110279\n","Epoch: 120 \tTraining Loss: 0.557897 \tValidation Loss: 0.110432\n","Epoch: 121 \tTraining Loss: 0.556245 \tValidation Loss: 0.111876\n","Epoch: 122 \tTraining Loss: 0.555678 \tValidation Loss: 0.109688\n","Epoch: 123 \tTraining Loss: 0.557369 \tValidation Loss: 0.113480\n","Epoch: 124 \tTraining Loss: 0.555312 \tValidation Loss: 0.109439\n","Epoch: 125 \tTraining Loss: 0.556402 \tValidation Loss: 0.112414\n","Epoch: 126 \tTraining Loss: 0.556500 \tValidation Loss: 0.111780\n","Epoch: 127 \tTraining Loss: 0.559272 \tValidation Loss: 0.109807\n","Epoch: 128 \tTraining Loss: 0.562309 \tValidation Loss: 0.114781\n","Epoch: 129 \tTraining Loss: 0.553622 \tValidation Loss: 0.107020\n","Epoch: 130 \tTraining Loss: 0.556581 \tValidation Loss: 0.111373\n","Epoch: 131 \tTraining Loss: 0.556263 \tValidation Loss: 0.111283\n","Epoch: 132 \tTraining Loss: 0.556879 \tValidation Loss: 0.112066\n","Epoch: 133 \tTraining Loss: 0.559044 \tValidation Loss: 0.111620\n","Epoch: 134 \tTraining Loss: 0.556713 \tValidation Loss: 0.108231\n","Epoch: 135 \tTraining Loss: 0.555913 \tValidation Loss: 0.114121\n","Epoch: 136 \tTraining Loss: 0.555899 \tValidation Loss: 0.108888\n","Epoch: 137 \tTraining Loss: 0.555952 \tValidation Loss: 0.110733\n","Epoch: 138 \tTraining Loss: 0.555919 \tValidation Loss: 0.111063\n","Epoch: 139 \tTraining Loss: 0.557693 \tValidation Loss: 0.110195\n","Epoch: 140 \tTraining Loss: 0.557879 \tValidation Loss: 0.113763\n","Epoch: 141 \tTraining Loss: 0.557960 \tValidation Loss: 0.107918\n","Epoch: 142 \tTraining Loss: 0.558546 \tValidation Loss: 0.112997\n","Epoch: 143 \tTraining Loss: 0.554988 \tValidation Loss: 0.108782\n","Epoch: 144 \tTraining Loss: 0.556027 \tValidation Loss: 0.113237\n","Epoch: 145 \tTraining Loss: 0.556700 \tValidation Loss: 0.111643\n","Epoch: 146 \tTraining Loss: 0.556132 \tValidation Loss: 0.108144\n","Epoch: 147 \tTraining Loss: 0.555178 \tValidation Loss: 0.111913\n","Epoch: 148 \tTraining Loss: 0.555613 \tValidation Loss: 0.110620\n","Epoch: 149 \tTraining Loss: 0.555239 \tValidation Loss: 0.109633\n","Epoch: 150 \tTraining Loss: 0.557058 \tValidation Loss: 0.108991\n","Epoch: 151 \tTraining Loss: 0.555832 \tValidation Loss: 0.111993\n","Epoch: 152 \tTraining Loss: 0.555652 \tValidation Loss: 0.109907\n","Epoch: 153 \tTraining Loss: 0.556029 \tValidation Loss: 0.110324\n","Epoch: 154 \tTraining Loss: 0.556179 \tValidation Loss: 0.112120\n","Epoch: 155 \tTraining Loss: 0.555078 \tValidation Loss: 0.108905\n","Epoch: 156 \tTraining Loss: 0.556784 \tValidation Loss: 0.111783\n","Epoch: 157 \tTraining Loss: 0.556582 \tValidation Loss: 0.110193\n","Epoch: 158 \tTraining Loss: 0.556416 \tValidation Loss: 0.111534\n","Epoch: 159 \tTraining Loss: 0.556786 \tValidation Loss: 0.109829\n","Epoch: 160 \tTraining Loss: 0.555979 \tValidation Loss: 0.111862\n","Epoch: 161 \tTraining Loss: 0.556053 \tValidation Loss: 0.110415\n","Epoch: 162 \tTraining Loss: 0.555209 \tValidation Loss: 0.111102\n","Epoch: 163 \tTraining Loss: 0.555652 \tValidation Loss: 0.111635\n","Epoch: 164 \tTraining Loss: 0.555066 \tValidation Loss: 0.109338\n","Epoch: 165 \tTraining Loss: 0.555501 \tValidation Loss: 0.111578\n","Epoch: 166 \tTraining Loss: 0.556693 \tValidation Loss: 0.109351\n","Epoch: 167 \tTraining Loss: 0.556039 \tValidation Loss: 0.112368\n","Epoch: 168 \tTraining Loss: 0.555746 \tValidation Loss: 0.110564\n","Epoch: 169 \tTraining Loss: 0.555952 \tValidation Loss: 0.110081\n","Epoch: 170 \tTraining Loss: 0.557107 \tValidation Loss: 0.110057\n","Epoch: 171 \tTraining Loss: 0.556808 \tValidation Loss: 0.109667\n","Epoch: 172 \tTraining Loss: 0.555608 \tValidation Loss: 0.113404\n","Epoch: 173 \tTraining Loss: 0.554856 \tValidation Loss: 0.109142\n","Epoch: 174 \tTraining Loss: 0.557471 \tValidation Loss: 0.111689\n","Epoch: 175 \tTraining Loss: 0.556781 \tValidation Loss: 0.112016\n","Epoch: 176 \tTraining Loss: 0.558169 \tValidation Loss: 0.109437\n","Epoch: 177 \tTraining Loss: 0.557617 \tValidation Loss: 0.112433\n","Epoch: 178 \tTraining Loss: 0.555937 \tValidation Loss: 0.108646\n","Epoch: 179 \tTraining Loss: 0.555607 \tValidation Loss: 0.112184\n","Epoch: 180 \tTraining Loss: 0.555406 \tValidation Loss: 0.110329\n","Epoch: 181 \tTraining Loss: 0.555607 \tValidation Loss: 0.108881\n","Epoch: 182 \tTraining Loss: 0.555326 \tValidation Loss: 0.110870\n","Epoch: 183 \tTraining Loss: 0.556328 \tValidation Loss: 0.110590\n","Epoch: 184 \tTraining Loss: 0.555076 \tValidation Loss: 0.111272\n","Epoch: 185 \tTraining Loss: 0.555762 \tValidation Loss: 0.110003\n","Epoch: 186 \tTraining Loss: 0.555935 \tValidation Loss: 0.110419\n","Epoch: 187 \tTraining Loss: 0.555773 \tValidation Loss: 0.111084\n","Epoch: 188 \tTraining Loss: 0.555079 \tValidation Loss: 0.109804\n","Epoch: 189 \tTraining Loss: 0.556842 \tValidation Loss: 0.112313\n","Epoch: 190 \tTraining Loss: 0.555373 \tValidation Loss: 0.110464\n","Epoch: 191 \tTraining Loss: 0.555624 \tValidation Loss: 0.110906\n","Epoch: 192 \tTraining Loss: 0.555861 \tValidation Loss: 0.111557\n","Epoch: 193 \tTraining Loss: 0.555614 \tValidation Loss: 0.109864\n","Epoch: 194 \tTraining Loss: 0.556167 \tValidation Loss: 0.110718\n","Epoch: 195 \tTraining Loss: 0.555589 \tValidation Loss: 0.110142\n","Epoch: 196 \tTraining Loss: 0.555022 \tValidation Loss: 0.111621\n","Epoch: 197 \tTraining Loss: 0.555431 \tValidation Loss: 0.110230\n","Epoch: 198 \tTraining Loss: 0.555177 \tValidation Loss: 0.111318\n","Epoch: 199 \tTraining Loss: 0.555042 \tValidation Loss: 0.110791\n","Epoch: 200 \tTraining Loss: 0.555435 \tValidation Loss: 0.110142\n","Epoch: 201 \tTraining Loss: 0.555556 \tValidation Loss: 0.110955\n","Epoch: 202 \tTraining Loss: 0.555595 \tValidation Loss: 0.110313\n","Epoch: 203 \tTraining Loss: 0.555394 \tValidation Loss: 0.111196\n","Epoch: 204 \tTraining Loss: 0.556095 \tValidation Loss: 0.110351\n","Epoch: 205 \tTraining Loss: 0.555828 \tValidation Loss: 0.112998\n","Epoch: 206 \tTraining Loss: 0.556722 \tValidation Loss: 0.108970\n","Epoch: 207 \tTraining Loss: 0.555637 \tValidation Loss: 0.111599\n","Epoch: 208 \tTraining Loss: 0.555775 \tValidation Loss: 0.111918\n","Epoch: 209 \tTraining Loss: 0.555650 \tValidation Loss: 0.109649\n","Epoch: 210 \tTraining Loss: 0.556102 \tValidation Loss: 0.110082\n","Epoch: 211 \tTraining Loss: 0.556129 \tValidation Loss: 0.112804\n","Epoch: 212 \tTraining Loss: 0.555479 \tValidation Loss: 0.108934\n","Epoch: 213 \tTraining Loss: 0.556576 \tValidation Loss: 0.112795\n","Epoch: 214 \tTraining Loss: 0.555702 \tValidation Loss: 0.110243\n","Epoch: 215 \tTraining Loss: 0.555569 \tValidation Loss: 0.109995\n","Epoch: 216 \tTraining Loss: 0.555766 \tValidation Loss: 0.112676\n","Epoch: 217 \tTraining Loss: 0.557565 \tValidation Loss: 0.109128\n","Epoch: 218 \tTraining Loss: 0.555262 \tValidation Loss: 0.112272\n","Epoch: 219 \tTraining Loss: 0.555642 \tValidation Loss: 0.110491\n","Epoch: 220 \tTraining Loss: 0.555848 \tValidation Loss: 0.111045\n","Epoch: 221 \tTraining Loss: 0.557024 \tValidation Loss: 0.111464\n","Epoch: 222 \tTraining Loss: 0.555117 \tValidation Loss: 0.109530\n","Epoch: 223 \tTraining Loss: 0.555374 \tValidation Loss: 0.111180\n","Epoch: 224 \tTraining Loss: 0.555445 \tValidation Loss: 0.112074\n","Epoch: 225 \tTraining Loss: 0.554468 \tValidation Loss: 0.108787\n","Epoch: 226 \tTraining Loss: 0.555397 \tValidation Loss: 0.110780\n","Epoch: 227 \tTraining Loss: 0.555920 \tValidation Loss: 0.112309\n","Epoch: 228 \tTraining Loss: 0.559003 \tValidation Loss: 0.109032\n","Epoch: 229 \tTraining Loss: 0.555670 \tValidation Loss: 0.112143\n","Epoch: 230 \tTraining Loss: 0.556389 \tValidation Loss: 0.111121\n","Epoch: 231 \tTraining Loss: 0.560183 \tValidation Loss: 0.110975\n","Epoch: 232 \tTraining Loss: 0.559045 \tValidation Loss: 0.109518\n","Epoch: 233 \tTraining Loss: 0.559878 \tValidation Loss: 0.114264\n","Epoch: 234 \tTraining Loss: 0.556516 \tValidation Loss: 0.109804\n","Epoch: 235 \tTraining Loss: 0.555842 \tValidation Loss: 0.112073\n","Epoch: 236 \tTraining Loss: 0.556361 \tValidation Loss: 0.109876\n","Epoch: 237 \tTraining Loss: 0.557630 \tValidation Loss: 0.109764\n","Epoch: 238 \tTraining Loss: 0.555524 \tValidation Loss: 0.111753\n","Epoch: 239 \tTraining Loss: 0.555663 \tValidation Loss: 0.110368\n","Epoch: 240 \tTraining Loss: 0.555038 \tValidation Loss: 0.110306\n","Epoch: 241 \tTraining Loss: 0.554945 \tValidation Loss: 0.110389\n","Epoch: 242 \tTraining Loss: 0.555500 \tValidation Loss: 0.110020\n","Epoch: 243 \tTraining Loss: 0.555122 \tValidation Loss: 0.111939\n","Epoch: 244 \tTraining Loss: 0.555863 \tValidation Loss: 0.109472\n","Epoch: 245 \tTraining Loss: 0.554839 \tValidation Loss: 0.110586\n","Epoch: 246 \tTraining Loss: 0.556560 \tValidation Loss: 0.112700\n","Epoch: 247 \tTraining Loss: 0.556704 \tValidation Loss: 0.108948\n","Epoch: 248 \tTraining Loss: 0.555379 \tValidation Loss: 0.111306\n","Epoch: 249 \tTraining Loss: 0.555896 \tValidation Loss: 0.111076\n","Epoch: 250 \tTraining Loss: 0.557382 \tValidation Loss: 0.112468\n","Epoch: 251 \tTraining Loss: 0.556319 \tValidation Loss: 0.108625\n","Epoch: 252 \tTraining Loss: 0.557106 \tValidation Loss: 0.112520\n","Epoch: 253 \tTraining Loss: 0.556303 \tValidation Loss: 0.109016\n","Epoch: 254 \tTraining Loss: 0.555570 \tValidation Loss: 0.111254\n","Epoch: 255 \tTraining Loss: 0.555449 \tValidation Loss: 0.110493\n","Epoch: 256 \tTraining Loss: 0.554996 \tValidation Loss: 0.111726\n","Epoch: 257 \tTraining Loss: 0.555159 \tValidation Loss: 0.109316\n","Epoch: 258 \tTraining Loss: 0.555315 \tValidation Loss: 0.111069\n","Epoch: 259 \tTraining Loss: 0.554979 \tValidation Loss: 0.111001\n","Epoch: 260 \tTraining Loss: 0.557083 \tValidation Loss: 0.109680\n","Epoch: 261 \tTraining Loss: 0.558788 \tValidation Loss: 0.113882\n","Epoch: 262 \tTraining Loss: 0.555912 \tValidation Loss: 0.109405\n","Epoch: 263 \tTraining Loss: 0.555258 \tValidation Loss: 0.111023\n","Epoch: 264 \tTraining Loss: 0.555129 \tValidation Loss: 0.111516\n","Epoch: 265 \tTraining Loss: 0.557060 \tValidation Loss: 0.108624\n","Epoch: 266 \tTraining Loss: 0.556020 \tValidation Loss: 0.112173\n","Epoch: 267 \tTraining Loss: 0.555878 \tValidation Loss: 0.109939\n","Epoch: 268 \tTraining Loss: 0.555641 \tValidation Loss: 0.110049\n","Epoch: 269 \tTraining Loss: 0.556135 \tValidation Loss: 0.110350\n","Epoch: 270 \tTraining Loss: 0.556079 \tValidation Loss: 0.111913\n","Epoch: 271 \tTraining Loss: 0.556198 \tValidation Loss: 0.109137\n","Epoch: 272 \tTraining Loss: 0.554759 \tValidation Loss: 0.111656\n","Epoch: 273 \tTraining Loss: 0.555339 \tValidation Loss: 0.110651\n","Epoch: 274 \tTraining Loss: 0.555168 \tValidation Loss: 0.111295\n","Epoch: 275 \tTraining Loss: 0.555326 \tValidation Loss: 0.109203\n","Epoch: 276 \tTraining Loss: 0.554670 \tValidation Loss: 0.111134\n","Epoch: 277 \tTraining Loss: 0.557285 \tValidation Loss: 0.112286\n","Epoch: 278 \tTraining Loss: 0.554604 \tValidation Loss: 0.108958\n","Epoch: 279 \tTraining Loss: 0.558079 \tValidation Loss: 0.111868\n","Epoch: 280 \tTraining Loss: 0.555217 \tValidation Loss: 0.111627\n","Epoch: 281 \tTraining Loss: 0.554771 \tValidation Loss: 0.110761\n","Epoch: 282 \tTraining Loss: 0.555155 \tValidation Loss: 0.110166\n","Epoch: 283 \tTraining Loss: 0.554953 \tValidation Loss: 0.110396\n","Epoch: 284 \tTraining Loss: 0.554785 \tValidation Loss: 0.110098\n","Epoch: 285 \tTraining Loss: 0.555894 \tValidation Loss: 0.111407\n","Epoch: 286 \tTraining Loss: 0.554845 \tValidation Loss: 0.111410\n","Epoch: 287 \tTraining Loss: 0.555155 \tValidation Loss: 0.110223\n","Epoch: 288 \tTraining Loss: 0.555021 \tValidation Loss: 0.110318\n","Epoch: 289 \tTraining Loss: 0.554609 \tValidation Loss: 0.110219\n","Epoch: 290 \tTraining Loss: 0.555265 \tValidation Loss: 0.110844\n","Epoch: 291 \tTraining Loss: 0.554841 \tValidation Loss: 0.110051\n","Epoch: 292 \tTraining Loss: 0.555282 \tValidation Loss: 0.110905\n","Epoch: 293 \tTraining Loss: 0.556458 \tValidation Loss: 0.109234\n","Epoch: 294 \tTraining Loss: 0.554632 \tValidation Loss: 0.111303\n","Epoch: 295 \tTraining Loss: 0.555358 \tValidation Loss: 0.110337\n","Epoch: 296 \tTraining Loss: 0.554429 \tValidation Loss: 0.111790\n","Epoch: 297 \tTraining Loss: 0.554757 \tValidation Loss: 0.110385\n","Epoch: 298 \tTraining Loss: 0.554595 \tValidation Loss: 0.111017\n","Epoch: 299 \tTraining Loss: 0.554464 \tValidation Loss: 0.109890\n","Epoch: 300 \tTraining Loss: 0.554586 \tValidation Loss: 0.110445\n","Epoch: 301 \tTraining Loss: 0.555449 \tValidation Loss: 0.110991\n","Epoch: 302 \tTraining Loss: 0.556379 \tValidation Loss: 0.109378\n","Epoch: 303 \tTraining Loss: 0.554446 \tValidation Loss: 0.112646\n","Epoch: 304 \tTraining Loss: 0.554670 \tValidation Loss: 0.110102\n","Epoch: 305 \tTraining Loss: 0.554871 \tValidation Loss: 0.109919\n","Epoch: 306 \tTraining Loss: 0.555788 \tValidation Loss: 0.108940\n","Epoch: 307 \tTraining Loss: 0.553730 \tValidation Loss: 0.111424\n","Epoch: 308 \tTraining Loss: 0.554793 \tValidation Loss: 0.111812\n","Epoch: 309 \tTraining Loss: 0.554653 \tValidation Loss: 0.108668\n","Epoch: 310 \tTraining Loss: 0.554167 \tValidation Loss: 0.111217\n","Epoch: 311 \tTraining Loss: 0.554081 \tValidation Loss: 0.110761\n","Epoch: 312 \tTraining Loss: 0.554028 \tValidation Loss: 0.110267\n","Epoch: 313 \tTraining Loss: 0.554582 \tValidation Loss: 0.109514\n","Epoch: 314 \tTraining Loss: 0.555041 \tValidation Loss: 0.112955\n","Epoch: 315 \tTraining Loss: 0.554515 \tValidation Loss: 0.109973\n","Epoch: 316 \tTraining Loss: 0.554302 \tValidation Loss: 0.111651\n","Epoch: 317 \tTraining Loss: 0.554164 \tValidation Loss: 0.109037\n","Epoch: 318 \tTraining Loss: 0.553683 \tValidation Loss: 0.111040\n","Epoch: 319 \tTraining Loss: 0.554284 \tValidation Loss: 0.111084\n","Epoch: 320 \tTraining Loss: 0.553317 \tValidation Loss: 0.109579\n","Epoch: 321 \tTraining Loss: 0.554545 \tValidation Loss: 0.110480\n","Epoch: 322 \tTraining Loss: 0.553712 \tValidation Loss: 0.111568\n","Epoch: 323 \tTraining Loss: 0.553282 \tValidation Loss: 0.108304\n","Epoch: 324 \tTraining Loss: 0.552395 \tValidation Loss: 0.111564\n","Epoch: 325 \tTraining Loss: 0.551748 \tValidation Loss: 0.110390\n","Epoch: 326 \tTraining Loss: 0.552242 \tValidation Loss: 0.110137\n","Epoch: 327 \tTraining Loss: 0.551550 \tValidation Loss: 0.108426\n","Epoch: 328 \tTraining Loss: 0.549930 \tValidation Loss: 0.109541\n","Epoch: 329 \tTraining Loss: 0.552014 \tValidation Loss: 0.107586\n","Epoch: 330 \tTraining Loss: 0.552583 \tValidation Loss: 0.109600\n","Epoch: 331 \tTraining Loss: 0.547003 \tValidation Loss: 0.109893\n","Epoch: 332 \tTraining Loss: 0.545526 \tValidation Loss: 0.108089\n","Epoch: 333 \tTraining Loss: 0.544419 \tValidation Loss: 0.108310\n","Epoch: 334 \tTraining Loss: 0.539446 \tValidation Loss: 0.108124\n","Epoch: 335 \tTraining Loss: 0.534555 \tValidation Loss: 0.105034\n","Epoch: 336 \tTraining Loss: 0.524692 \tValidation Loss: 0.108009\n","Epoch: 337 \tTraining Loss: 0.513807 \tValidation Loss: 0.103093\n","Validation loss decreased (0.103540 --> 0.103093).  Saving model ...\n","Epoch: 338 \tTraining Loss: 0.493592 \tValidation Loss: 0.093766\n","Validation loss decreased (0.103093 --> 0.093766).  Saving model ...\n","Epoch: 339 \tTraining Loss: 0.471059 \tValidation Loss: 0.090214\n","Validation loss decreased (0.093766 --> 0.090214).  Saving model ...\n","Epoch: 340 \tTraining Loss: 0.442993 \tValidation Loss: 0.091421\n","Epoch: 341 \tTraining Loss: 0.427458 \tValidation Loss: 0.083311\n","Validation loss decreased (0.090214 --> 0.083311).  Saving model ...\n","Epoch: 342 \tTraining Loss: 0.399856 \tValidation Loss: 0.072004\n","Validation loss decreased (0.083311 --> 0.072004).  Saving model ...\n","Epoch: 343 \tTraining Loss: 0.378256 \tValidation Loss: 0.065983\n","Validation loss decreased (0.072004 --> 0.065983).  Saving model ...\n","Epoch: 344 \tTraining Loss: 0.344468 \tValidation Loss: 0.062917\n","Validation loss decreased (0.065983 --> 0.062917).  Saving model ...\n","Epoch: 345 \tTraining Loss: 0.359490 \tValidation Loss: 0.079350\n","Epoch: 346 \tTraining Loss: 0.327662 \tValidation Loss: 0.070764\n","Epoch: 347 \tTraining Loss: 0.283599 \tValidation Loss: 0.056479\n","Validation loss decreased (0.062917 --> 0.056479).  Saving model ...\n","Epoch: 348 \tTraining Loss: 0.282711 \tValidation Loss: 0.054597\n","Validation loss decreased (0.056479 --> 0.054597).  Saving model ...\n","Epoch: 349 \tTraining Loss: 0.299270 \tValidation Loss: 0.056769\n","Epoch: 350 \tTraining Loss: 0.239571 \tValidation Loss: 0.059614\n","Epoch: 351 \tTraining Loss: 0.210893 \tValidation Loss: 0.047981\n","Validation loss decreased (0.054597 --> 0.047981).  Saving model ...\n","Epoch: 352 \tTraining Loss: 0.189995 \tValidation Loss: 0.066572\n","Epoch: 353 \tTraining Loss: 0.172120 \tValidation Loss: 0.048392\n","Epoch: 354 \tTraining Loss: 0.164323 \tValidation Loss: 0.059876\n","Epoch: 355 \tTraining Loss: 0.148728 \tValidation Loss: 0.051376\n","Epoch: 356 \tTraining Loss: 0.126593 \tValidation Loss: 0.044253\n","Validation loss decreased (0.047981 --> 0.044253).  Saving model ...\n","Epoch: 357 \tTraining Loss: 0.114227 \tValidation Loss: 0.046447\n","Epoch: 358 \tTraining Loss: 0.101926 \tValidation Loss: 0.047612\n","Epoch: 359 \tTraining Loss: 0.096644 \tValidation Loss: 0.055504\n","Epoch: 360 \tTraining Loss: 0.077332 \tValidation Loss: 0.050380\n","Epoch: 361 \tTraining Loss: 0.064681 \tValidation Loss: 0.047136\n","Epoch: 362 \tTraining Loss: 0.055393 \tValidation Loss: 0.045245\n","Epoch: 363 \tTraining Loss: 0.045612 \tValidation Loss: 0.048822\n","Epoch: 364 \tTraining Loss: 0.039494 \tValidation Loss: 0.048557\n","Epoch: 365 \tTraining Loss: 0.034201 \tValidation Loss: 0.049407\n","Epoch: 366 \tTraining Loss: 0.029324 \tValidation Loss: 0.047781\n","Epoch: 367 \tTraining Loss: 0.026057 \tValidation Loss: 0.051669\n","Epoch: 368 \tTraining Loss: 0.022799 \tValidation Loss: 0.053567\n","Epoch: 369 \tTraining Loss: 0.020245 \tValidation Loss: 0.056141\n","Epoch: 370 \tTraining Loss: 0.017921 \tValidation Loss: 0.056481\n","Epoch: 371 \tTraining Loss: 0.015115 \tValidation Loss: 0.060264\n","Epoch: 372 \tTraining Loss: 0.013690 \tValidation Loss: 0.056411\n","Epoch: 373 \tTraining Loss: 0.009605 \tValidation Loss: 0.060812\n","Epoch: 374 \tTraining Loss: 0.008505 \tValidation Loss: 0.064766\n","Epoch: 375 \tTraining Loss: 0.006727 \tValidation Loss: 0.064554\n","Epoch: 376 \tTraining Loss: 0.005660 \tValidation Loss: 0.062158\n","Epoch: 377 \tTraining Loss: 0.004966 \tValidation Loss: 0.064762\n","Epoch: 378 \tTraining Loss: 0.004428 \tValidation Loss: 0.064240\n","Epoch: 379 \tTraining Loss: 0.003946 \tValidation Loss: 0.064818\n","Epoch: 380 \tTraining Loss: 0.003542 \tValidation Loss: 0.067470\n","Epoch: 381 \tTraining Loss: 0.003142 \tValidation Loss: 0.066955\n","Epoch: 382 \tTraining Loss: 0.002850 \tValidation Loss: 0.070548\n","Epoch: 383 \tTraining Loss: 0.002627 \tValidation Loss: 0.070145\n","Epoch: 384 \tTraining Loss: 0.002398 \tValidation Loss: 0.071418\n","Epoch: 385 \tTraining Loss: 0.002276 \tValidation Loss: 0.073759\n","Epoch: 386 \tTraining Loss: 0.002081 \tValidation Loss: 0.071205\n","Epoch: 387 \tTraining Loss: 0.001960 \tValidation Loss: 0.073565\n","Epoch: 388 \tTraining Loss: 0.001844 \tValidation Loss: 0.075766\n","Epoch: 389 \tTraining Loss: 0.001696 \tValidation Loss: 0.073389\n","Epoch: 390 \tTraining Loss: 0.001582 \tValidation Loss: 0.077136\n","Epoch: 391 \tTraining Loss: 0.001511 \tValidation Loss: 0.075250\n","Epoch: 392 \tTraining Loss: 0.001395 \tValidation Loss: 0.076881\n","Epoch: 393 \tTraining Loss: 0.001309 \tValidation Loss: 0.077166\n","Epoch: 394 \tTraining Loss: 0.001237 \tValidation Loss: 0.077842\n","Epoch: 395 \tTraining Loss: 0.001182 \tValidation Loss: 0.079033\n","Epoch: 396 \tTraining Loss: 0.001115 \tValidation Loss: 0.078108\n","Epoch: 397 \tTraining Loss: 0.001055 \tValidation Loss: 0.079203\n","Epoch: 398 \tTraining Loss: 0.001004 \tValidation Loss: 0.079017\n","Epoch: 399 \tTraining Loss: 0.000955 \tValidation Loss: 0.080294\n","Epoch: 400 \tTraining Loss: 0.000924 \tValidation Loss: 0.079979\n","Epoch: 401 \tTraining Loss: 0.000880 \tValidation Loss: 0.080866\n","Epoch: 402 \tTraining Loss: 0.000840 \tValidation Loss: 0.081639\n","Epoch: 403 \tTraining Loss: 0.000807 \tValidation Loss: 0.081190\n","Epoch: 404 \tTraining Loss: 0.000776 \tValidation Loss: 0.082294\n","Epoch: 405 \tTraining Loss: 0.000740 \tValidation Loss: 0.082776\n","Epoch: 406 \tTraining Loss: 0.000708 \tValidation Loss: 0.082579\n","Epoch: 407 \tTraining Loss: 0.000681 \tValidation Loss: 0.083641\n","Epoch: 408 \tTraining Loss: 0.000654 \tValidation Loss: 0.084650\n","Epoch: 409 \tTraining Loss: 0.000631 \tValidation Loss: 0.083891\n","Epoch: 410 \tTraining Loss: 0.000608 \tValidation Loss: 0.085250\n","Epoch: 411 \tTraining Loss: 0.000588 \tValidation Loss: 0.084334\n","Epoch: 412 \tTraining Loss: 0.000561 \tValidation Loss: 0.086146\n","Epoch: 413 \tTraining Loss: 0.000544 \tValidation Loss: 0.085433\n","Epoch: 414 \tTraining Loss: 0.000525 \tValidation Loss: 0.086062\n","Epoch: 415 \tTraining Loss: 0.000508 \tValidation Loss: 0.086735\n","Epoch: 416 \tTraining Loss: 0.000491 \tValidation Loss: 0.086742\n","Epoch: 417 \tTraining Loss: 0.000483 \tValidation Loss: 0.087266\n","Epoch: 418 \tTraining Loss: 0.000471 \tValidation Loss: 0.087227\n","Epoch: 419 \tTraining Loss: 0.000447 \tValidation Loss: 0.087714\n","Epoch: 420 \tTraining Loss: 0.000432 \tValidation Loss: 0.088016\n","Epoch: 421 \tTraining Loss: 0.000422 \tValidation Loss: 0.087997\n","Epoch: 422 \tTraining Loss: 0.000408 \tValidation Loss: 0.089072\n","Epoch: 423 \tTraining Loss: 0.000397 \tValidation Loss: 0.088771\n","Epoch: 424 \tTraining Loss: 0.000384 \tValidation Loss: 0.089518\n","Epoch: 425 \tTraining Loss: 0.000374 \tValidation Loss: 0.089405\n","Epoch: 426 \tTraining Loss: 0.000363 \tValidation Loss: 0.089554\n","Epoch: 427 \tTraining Loss: 0.000354 \tValidation Loss: 0.089886\n","Epoch: 428 \tTraining Loss: 0.000348 \tValidation Loss: 0.090348\n","Epoch: 429 \tTraining Loss: 0.000336 \tValidation Loss: 0.090069\n","Epoch: 430 \tTraining Loss: 0.000327 \tValidation Loss: 0.090988\n","Epoch: 431 \tTraining Loss: 0.000321 \tValidation Loss: 0.091153\n","Epoch: 432 \tTraining Loss: 0.000311 \tValidation Loss: 0.091132\n","Epoch: 433 \tTraining Loss: 0.000304 \tValidation Loss: 0.091493\n","Epoch: 434 \tTraining Loss: 0.000296 \tValidation Loss: 0.091870\n","Epoch: 435 \tTraining Loss: 0.000287 \tValidation Loss: 0.091963\n","Epoch: 436 \tTraining Loss: 0.000283 \tValidation Loss: 0.092487\n","Epoch: 437 \tTraining Loss: 0.000275 \tValidation Loss: 0.092547\n","Epoch: 438 \tTraining Loss: 0.000268 \tValidation Loss: 0.092480\n","Epoch: 439 \tTraining Loss: 0.000261 \tValidation Loss: 0.093261\n","Epoch: 440 \tTraining Loss: 0.000255 \tValidation Loss: 0.093190\n","Epoch: 441 \tTraining Loss: 0.000249 \tValidation Loss: 0.093526\n","Epoch: 442 \tTraining Loss: 0.000244 \tValidation Loss: 0.093886\n","Epoch: 443 \tTraining Loss: 0.000239 \tValidation Loss: 0.093782\n","Epoch: 444 \tTraining Loss: 0.000233 \tValidation Loss: 0.094245\n","Epoch: 445 \tTraining Loss: 0.000228 \tValidation Loss: 0.094276\n","Epoch: 446 \tTraining Loss: 0.000223 \tValidation Loss: 0.095009\n","Epoch: 447 \tTraining Loss: 0.000218 \tValidation Loss: 0.094563\n","Epoch: 448 \tTraining Loss: 0.000214 \tValidation Loss: 0.094745\n","Epoch: 449 \tTraining Loss: 0.000209 \tValidation Loss: 0.095354\n","Epoch: 450 \tTraining Loss: 0.000205 \tValidation Loss: 0.095413\n","Epoch: 451 \tTraining Loss: 0.000202 \tValidation Loss: 0.095604\n","Epoch: 452 \tTraining Loss: 0.000197 \tValidation Loss: 0.095960\n","Epoch: 453 \tTraining Loss: 0.000193 \tValidation Loss: 0.095737\n","Epoch: 454 \tTraining Loss: 0.000190 \tValidation Loss: 0.096207\n","Epoch: 455 \tTraining Loss: 0.000186 \tValidation Loss: 0.096761\n","Epoch: 456 \tTraining Loss: 0.000182 \tValidation Loss: 0.096501\n","Epoch: 457 \tTraining Loss: 0.000179 \tValidation Loss: 0.096844\n","Epoch: 458 \tTraining Loss: 0.000175 \tValidation Loss: 0.097233\n","Epoch: 459 \tTraining Loss: 0.000172 \tValidation Loss: 0.097161\n","Epoch: 460 \tTraining Loss: 0.000169 \tValidation Loss: 0.097558\n","Epoch: 461 \tTraining Loss: 0.000166 \tValidation Loss: 0.097320\n","Epoch: 462 \tTraining Loss: 0.000162 \tValidation Loss: 0.098278\n","Epoch: 463 \tTraining Loss: 0.000159 \tValidation Loss: 0.097941\n","Epoch: 464 \tTraining Loss: 0.000157 \tValidation Loss: 0.097981\n","Epoch: 465 \tTraining Loss: 0.000154 \tValidation Loss: 0.098613\n","Epoch: 466 \tTraining Loss: 0.000151 \tValidation Loss: 0.098621\n","Epoch: 467 \tTraining Loss: 0.000148 \tValidation Loss: 0.098389\n","Epoch: 468 \tTraining Loss: 0.000146 \tValidation Loss: 0.098791\n","Epoch: 469 \tTraining Loss: 0.000143 \tValidation Loss: 0.099067\n","Epoch: 470 \tTraining Loss: 0.000141 \tValidation Loss: 0.099096\n","Epoch: 471 \tTraining Loss: 0.000138 \tValidation Loss: 0.099426\n","Epoch: 472 \tTraining Loss: 0.000136 \tValidation Loss: 0.099563\n","Epoch: 473 \tTraining Loss: 0.000134 \tValidation Loss: 0.099388\n","Epoch: 474 \tTraining Loss: 0.000132 \tValidation Loss: 0.100100\n","Epoch: 475 \tTraining Loss: 0.000129 \tValidation Loss: 0.099739\n","Epoch: 476 \tTraining Loss: 0.000127 \tValidation Loss: 0.100110\n","Epoch: 477 \tTraining Loss: 0.000125 \tValidation Loss: 0.100470\n","Epoch: 478 \tTraining Loss: 0.000124 \tValidation Loss: 0.100179\n","Epoch: 479 \tTraining Loss: 0.000121 \tValidation Loss: 0.101194\n","Epoch: 480 \tTraining Loss: 0.000119 \tValidation Loss: 0.100730\n","Epoch: 481 \tTraining Loss: 0.000117 \tValidation Loss: 0.100889\n","Epoch: 482 \tTraining Loss: 0.000116 \tValidation Loss: 0.101144\n","Epoch: 483 \tTraining Loss: 0.000113 \tValidation Loss: 0.101018\n","Epoch: 484 \tTraining Loss: 0.000112 \tValidation Loss: 0.101544\n","Epoch: 485 \tTraining Loss: 0.000110 \tValidation Loss: 0.101623\n","Epoch: 486 \tTraining Loss: 0.000108 \tValidation Loss: 0.101445\n","Epoch: 487 \tTraining Loss: 0.000106 \tValidation Loss: 0.101825\n","Epoch: 488 \tTraining Loss: 0.000105 \tValidation Loss: 0.102250\n","Epoch: 489 \tTraining Loss: 0.000104 \tValidation Loss: 0.102213\n","Epoch: 490 \tTraining Loss: 0.000102 \tValidation Loss: 0.102181\n","Epoch: 491 \tTraining Loss: 0.000101 \tValidation Loss: 0.102410\n","Epoch: 492 \tTraining Loss: 0.000099 \tValidation Loss: 0.102446\n","Epoch: 493 \tTraining Loss: 0.000097 \tValidation Loss: 0.103268\n","Epoch: 494 \tTraining Loss: 0.000096 \tValidation Loss: 0.102938\n","Epoch: 495 \tTraining Loss: 0.000095 \tValidation Loss: 0.102944\n","Epoch: 496 \tTraining Loss: 0.000093 \tValidation Loss: 0.103240\n","Epoch: 497 \tTraining Loss: 0.000092 \tValidation Loss: 0.103420\n","Epoch: 498 \tTraining Loss: 0.000091 \tValidation Loss: 0.103147\n","Epoch: 499 \tTraining Loss: 0.000090 \tValidation Loss: 0.104067\n","Epoch: 500 \tTraining Loss: 0.000089 \tValidation Loss: 0.103253\n","Epoch: 501 \tTraining Loss: 0.000087 \tValidation Loss: 0.104462\n","Epoch: 502 \tTraining Loss: 0.000086 \tValidation Loss: 0.103903\n","Epoch: 503 \tTraining Loss: 0.000084 \tValidation Loss: 0.104021\n","Epoch: 504 \tTraining Loss: 0.000083 \tValidation Loss: 0.104570\n","Epoch: 505 \tTraining Loss: 0.000082 \tValidation Loss: 0.104414\n","Epoch: 506 \tTraining Loss: 0.000081 \tValidation Loss: 0.104555\n","Epoch: 507 \tTraining Loss: 0.000080 \tValidation Loss: 0.104717\n","Epoch: 508 \tTraining Loss: 0.000079 \tValidation Loss: 0.104819\n","Epoch: 509 \tTraining Loss: 0.000078 \tValidation Loss: 0.105151\n","Epoch: 510 \tTraining Loss: 0.000077 \tValidation Loss: 0.105246\n","Epoch: 511 \tTraining Loss: 0.000076 \tValidation Loss: 0.104658\n","Epoch: 512 \tTraining Loss: 0.000075 \tValidation Loss: 0.105662\n","Epoch: 513 \tTraining Loss: 0.000073 \tValidation Loss: 0.105558\n","Epoch: 514 \tTraining Loss: 0.000072 \tValidation Loss: 0.105574\n","Epoch: 515 \tTraining Loss: 0.000072 \tValidation Loss: 0.105904\n","Epoch: 516 \tTraining Loss: 0.000071 \tValidation Loss: 0.105870\n","Epoch: 517 \tTraining Loss: 0.000070 \tValidation Loss: 0.106152\n","Epoch: 518 \tTraining Loss: 0.000069 \tValidation Loss: 0.105998\n","Epoch: 519 \tTraining Loss: 0.000068 \tValidation Loss: 0.106467\n","Epoch: 520 \tTraining Loss: 0.000067 \tValidation Loss: 0.106479\n","Epoch: 521 \tTraining Loss: 0.000066 \tValidation Loss: 0.106311\n","Epoch: 522 \tTraining Loss: 0.000065 \tValidation Loss: 0.106594\n","Epoch: 523 \tTraining Loss: 0.000065 \tValidation Loss: 0.106722\n","Epoch: 524 \tTraining Loss: 0.000064 \tValidation Loss: 0.107026\n","Epoch: 525 \tTraining Loss: 0.000063 \tValidation Loss: 0.107215\n","Epoch: 526 \tTraining Loss: 0.000062 \tValidation Loss: 0.107129\n","Epoch: 527 \tTraining Loss: 0.000062 \tValidation Loss: 0.107516\n","Epoch: 528 \tTraining Loss: 0.000061 \tValidation Loss: 0.107218\n","Epoch: 529 \tTraining Loss: 0.000060 \tValidation Loss: 0.107665\n","Epoch: 530 \tTraining Loss: 0.000059 \tValidation Loss: 0.107707\n","Epoch: 531 \tTraining Loss: 0.000058 \tValidation Loss: 0.107720\n","Epoch: 532 \tTraining Loss: 0.000058 \tValidation Loss: 0.107788\n","Epoch: 533 \tTraining Loss: 0.000057 \tValidation Loss: 0.108137\n","Epoch: 534 \tTraining Loss: 0.000056 \tValidation Loss: 0.108329\n","Epoch: 535 \tTraining Loss: 0.000056 \tValidation Loss: 0.108316\n","Epoch: 536 \tTraining Loss: 0.000055 \tValidation Loss: 0.108448\n","Epoch: 537 \tTraining Loss: 0.000054 \tValidation Loss: 0.108304\n","Epoch: 538 \tTraining Loss: 0.000054 \tValidation Loss: 0.108694\n","Epoch: 539 \tTraining Loss: 0.000053 \tValidation Loss: 0.109055\n","Epoch: 540 \tTraining Loss: 0.000053 \tValidation Loss: 0.108969\n","Epoch: 541 \tTraining Loss: 0.000052 \tValidation Loss: 0.108683\n","Epoch: 542 \tTraining Loss: 0.000051 \tValidation Loss: 0.109209\n","Epoch: 543 \tTraining Loss: 0.000051 \tValidation Loss: 0.109310\n","Epoch: 544 \tTraining Loss: 0.000050 \tValidation Loss: 0.109178\n","Epoch: 545 \tTraining Loss: 0.000050 \tValidation Loss: 0.109877\n","Epoch: 546 \tTraining Loss: 0.000049 \tValidation Loss: 0.109319\n","Epoch: 547 \tTraining Loss: 0.000048 \tValidation Loss: 0.109670\n","Epoch: 548 \tTraining Loss: 0.000048 \tValidation Loss: 0.110164\n","Epoch: 549 \tTraining Loss: 0.000047 \tValidation Loss: 0.109663\n","Epoch: 550 \tTraining Loss: 0.000047 \tValidation Loss: 0.110028\n","Epoch: 551 \tTraining Loss: 0.000046 \tValidation Loss: 0.110321\n","Epoch: 552 \tTraining Loss: 0.000046 \tValidation Loss: 0.110091\n","Epoch: 553 \tTraining Loss: 0.000045 \tValidation Loss: 0.110361\n","Epoch: 554 \tTraining Loss: 0.000045 \tValidation Loss: 0.110437\n","Epoch: 555 \tTraining Loss: 0.000044 \tValidation Loss: 0.110391\n","Epoch: 556 \tTraining Loss: 0.000044 \tValidation Loss: 0.111034\n","Epoch: 557 \tTraining Loss: 0.000043 \tValidation Loss: 0.110525\n","Epoch: 558 \tTraining Loss: 0.000043 \tValidation Loss: 0.111066\n","Epoch: 559 \tTraining Loss: 0.000042 \tValidation Loss: 0.111132\n","Epoch: 560 \tTraining Loss: 0.000042 \tValidation Loss: 0.111121\n","Epoch: 561 \tTraining Loss: 0.000041 \tValidation Loss: 0.111106\n","Epoch: 562 \tTraining Loss: 0.000041 \tValidation Loss: 0.111251\n","Epoch: 563 \tTraining Loss: 0.000040 \tValidation Loss: 0.111463\n","Epoch: 564 \tTraining Loss: 0.000040 \tValidation Loss: 0.111985\n","Epoch: 565 \tTraining Loss: 0.000040 \tValidation Loss: 0.111349\n","Epoch: 566 \tTraining Loss: 0.000039 \tValidation Loss: 0.112074\n","Epoch: 567 \tTraining Loss: 0.000039 \tValidation Loss: 0.111589\n","Epoch: 568 \tTraining Loss: 0.000038 \tValidation Loss: 0.112206\n","Epoch: 569 \tTraining Loss: 0.000038 \tValidation Loss: 0.112252\n","Epoch: 570 \tTraining Loss: 0.000037 \tValidation Loss: 0.111899\n","Epoch: 571 \tTraining Loss: 0.000037 \tValidation Loss: 0.112416\n","Epoch: 572 \tTraining Loss: 0.000037 \tValidation Loss: 0.112351\n","Epoch: 573 \tTraining Loss: 0.000036 \tValidation Loss: 0.112502\n","Epoch: 574 \tTraining Loss: 0.000036 \tValidation Loss: 0.112874\n","Epoch: 575 \tTraining Loss: 0.000036 \tValidation Loss: 0.112588\n","Epoch: 576 \tTraining Loss: 0.000035 \tValidation Loss: 0.113081\n","Epoch: 577 \tTraining Loss: 0.000035 \tValidation Loss: 0.112918\n","Epoch: 578 \tTraining Loss: 0.000034 \tValidation Loss: 0.112944\n","Epoch: 579 \tTraining Loss: 0.000034 \tValidation Loss: 0.113338\n","Epoch: 580 \tTraining Loss: 0.000034 \tValidation Loss: 0.112957\n","Epoch: 581 \tTraining Loss: 0.000033 \tValidation Loss: 0.113529\n","Epoch: 582 \tTraining Loss: 0.000033 \tValidation Loss: 0.113221\n","Epoch: 583 \tTraining Loss: 0.000033 \tValidation Loss: 0.113659\n","Epoch: 584 \tTraining Loss: 0.000032 \tValidation Loss: 0.113945\n","Epoch: 585 \tTraining Loss: 0.000032 \tValidation Loss: 0.113781\n","Epoch: 586 \tTraining Loss: 0.000032 \tValidation Loss: 0.113612\n","Epoch: 587 \tTraining Loss: 0.000031 \tValidation Loss: 0.113952\n","Epoch: 588 \tTraining Loss: 0.000031 \tValidation Loss: 0.114051\n","Epoch: 589 \tTraining Loss: 0.000031 \tValidation Loss: 0.114041\n","Epoch: 590 \tTraining Loss: 0.000030 \tValidation Loss: 0.114499\n","Epoch: 591 \tTraining Loss: 0.000030 \tValidation Loss: 0.114115\n","Epoch: 592 \tTraining Loss: 0.000030 \tValidation Loss: 0.114666\n","Epoch: 593 \tTraining Loss: 0.000029 \tValidation Loss: 0.114593\n","Epoch: 594 \tTraining Loss: 0.000029 \tValidation Loss: 0.114620\n","Epoch: 595 \tTraining Loss: 0.000029 \tValidation Loss: 0.114773\n","Epoch: 596 \tTraining Loss: 0.000029 \tValidation Loss: 0.114781\n","Epoch: 597 \tTraining Loss: 0.000028 \tValidation Loss: 0.114744\n","Epoch: 598 \tTraining Loss: 0.000028 \tValidation Loss: 0.114909\n","Epoch: 599 \tTraining Loss: 0.000028 \tValidation Loss: 0.115363\n","Epoch: 600 \tTraining Loss: 0.000028 \tValidation Loss: 0.115029\n","Epoch: 601 \tTraining Loss: 0.000027 \tValidation Loss: 0.115537\n","Epoch: 602 \tTraining Loss: 0.000027 \tValidation Loss: 0.115233\n","Epoch: 603 \tTraining Loss: 0.000027 \tValidation Loss: 0.115509\n","Epoch: 604 \tTraining Loss: 0.000026 \tValidation Loss: 0.115492\n","Epoch: 605 \tTraining Loss: 0.000026 \tValidation Loss: 0.115630\n","Epoch: 606 \tTraining Loss: 0.000026 \tValidation Loss: 0.116027\n","Epoch: 607 \tTraining Loss: 0.000026 \tValidation Loss: 0.115918\n","Epoch: 608 \tTraining Loss: 0.000025 \tValidation Loss: 0.115806\n","Epoch: 609 \tTraining Loss: 0.000025 \tValidation Loss: 0.116194\n","Epoch: 610 \tTraining Loss: 0.000025 \tValidation Loss: 0.116119\n","Epoch: 611 \tTraining Loss: 0.000025 \tValidation Loss: 0.116387\n","Epoch: 612 \tTraining Loss: 0.000024 \tValidation Loss: 0.116313\n","Epoch: 613 \tTraining Loss: 0.000024 \tValidation Loss: 0.116450\n","Epoch: 614 \tTraining Loss: 0.000024 \tValidation Loss: 0.116819\n","Epoch: 615 \tTraining Loss: 0.000024 \tValidation Loss: 0.116411\n","Epoch: 616 \tTraining Loss: 0.000024 \tValidation Loss: 0.116556\n","Epoch: 617 \tTraining Loss: 0.000023 \tValidation Loss: 0.117148\n","Epoch: 618 \tTraining Loss: 0.000023 \tValidation Loss: 0.116845\n","Epoch: 619 \tTraining Loss: 0.000023 \tValidation Loss: 0.116758\n","Epoch: 620 \tTraining Loss: 0.000023 \tValidation Loss: 0.117322\n","Epoch: 621 \tTraining Loss: 0.000022 \tValidation Loss: 0.117087\n","Epoch: 622 \tTraining Loss: 0.000022 \tValidation Loss: 0.117479\n","Epoch: 623 \tTraining Loss: 0.000022 \tValidation Loss: 0.117229\n","Epoch: 624 \tTraining Loss: 0.000022 \tValidation Loss: 0.117489\n","Epoch: 625 \tTraining Loss: 0.000022 \tValidation Loss: 0.117760\n","Epoch: 626 \tTraining Loss: 0.000021 \tValidation Loss: 0.117468\n","Epoch: 627 \tTraining Loss: 0.000021 \tValidation Loss: 0.117631\n","Epoch: 628 \tTraining Loss: 0.000021 \tValidation Loss: 0.118008\n","Epoch: 629 \tTraining Loss: 0.000021 \tValidation Loss: 0.117920\n","Epoch: 630 \tTraining Loss: 0.000021 \tValidation Loss: 0.118130\n","Epoch: 631 \tTraining Loss: 0.000020 \tValidation Loss: 0.117941\n","Epoch: 632 \tTraining Loss: 0.000020 \tValidation Loss: 0.118172\n","Epoch: 633 \tTraining Loss: 0.000020 \tValidation Loss: 0.118507\n","Epoch: 634 \tTraining Loss: 0.000020 \tValidation Loss: 0.118336\n","Epoch: 635 \tTraining Loss: 0.000020 \tValidation Loss: 0.118626\n","Epoch: 636 \tTraining Loss: 0.000020 \tValidation Loss: 0.118396\n","Epoch: 637 \tTraining Loss: 0.000019 \tValidation Loss: 0.118602\n","Epoch: 638 \tTraining Loss: 0.000019 \tValidation Loss: 0.119098\n","Epoch: 639 \tTraining Loss: 0.000019 \tValidation Loss: 0.118756\n","Epoch: 640 \tTraining Loss: 0.000019 \tValidation Loss: 0.118643\n","Epoch: 641 \tTraining Loss: 0.000019 \tValidation Loss: 0.119349\n","Epoch: 642 \tTraining Loss: 0.000019 \tValidation Loss: 0.119037\n","Epoch: 643 \tTraining Loss: 0.000018 \tValidation Loss: 0.119561\n","Epoch: 644 \tTraining Loss: 0.000018 \tValidation Loss: 0.119180\n","Epoch: 645 \tTraining Loss: 0.000018 \tValidation Loss: 0.119481\n","Epoch: 646 \tTraining Loss: 0.000018 \tValidation Loss: 0.119463\n","Epoch: 647 \tTraining Loss: 0.000018 \tValidation Loss: 0.119611\n","Epoch: 648 \tTraining Loss: 0.000018 \tValidation Loss: 0.119398\n","Epoch: 649 \tTraining Loss: 0.000017 \tValidation Loss: 0.119989\n","Epoch: 650 \tTraining Loss: 0.000017 \tValidation Loss: 0.119623\n","Epoch: 651 \tTraining Loss: 0.000017 \tValidation Loss: 0.119830\n","Epoch: 652 \tTraining Loss: 0.000017 \tValidation Loss: 0.120308\n","Epoch: 653 \tTraining Loss: 0.000017 \tValidation Loss: 0.119975\n","Epoch: 654 \tTraining Loss: 0.000017 \tValidation Loss: 0.119880\n","Epoch: 655 \tTraining Loss: 0.000016 \tValidation Loss: 0.120353\n","Epoch: 656 \tTraining Loss: 0.000016 \tValidation Loss: 0.120475\n","Epoch: 657 \tTraining Loss: 0.000016 \tValidation Loss: 0.120537\n","Epoch: 658 \tTraining Loss: 0.000016 \tValidation Loss: 0.120733\n","Epoch: 659 \tTraining Loss: 0.000016 \tValidation Loss: 0.120511\n","Epoch: 660 \tTraining Loss: 0.000016 \tValidation Loss: 0.120691\n","Epoch: 661 \tTraining Loss: 0.000016 \tValidation Loss: 0.120796\n","Epoch: 662 \tTraining Loss: 0.000016 \tValidation Loss: 0.120935\n","Epoch: 663 \tTraining Loss: 0.000015 \tValidation Loss: 0.120902\n","Epoch: 664 \tTraining Loss: 0.000015 \tValidation Loss: 0.121139\n","Epoch: 665 \tTraining Loss: 0.000015 \tValidation Loss: 0.121358\n","Epoch: 666 \tTraining Loss: 0.000015 \tValidation Loss: 0.121039\n","Epoch: 667 \tTraining Loss: 0.000015 \tValidation Loss: 0.121373\n","Epoch: 668 \tTraining Loss: 0.000015 \tValidation Loss: 0.121206\n","Epoch: 669 \tTraining Loss: 0.000015 \tValidation Loss: 0.121578\n","Epoch: 670 \tTraining Loss: 0.000014 \tValidation Loss: 0.121659\n","Epoch: 671 \tTraining Loss: 0.000014 \tValidation Loss: 0.121512\n","Epoch: 672 \tTraining Loss: 0.000014 \tValidation Loss: 0.121934\n","Epoch: 673 \tTraining Loss: 0.000014 \tValidation Loss: 0.121581\n","Epoch: 674 \tTraining Loss: 0.000014 \tValidation Loss: 0.121886\n","Epoch: 675 \tTraining Loss: 0.000014 \tValidation Loss: 0.122330\n","Epoch: 676 \tTraining Loss: 0.000014 \tValidation Loss: 0.122000\n","Epoch: 677 \tTraining Loss: 0.000014 \tValidation Loss: 0.122258\n","Epoch: 678 \tTraining Loss: 0.000014 \tValidation Loss: 0.122126\n","Epoch: 679 \tTraining Loss: 0.000013 \tValidation Loss: 0.122127\n","Epoch: 680 \tTraining Loss: 0.000013 \tValidation Loss: 0.122640\n","Epoch: 681 \tTraining Loss: 0.000013 \tValidation Loss: 0.122804\n","Epoch: 682 \tTraining Loss: 0.000013 \tValidation Loss: 0.122377\n","Epoch: 683 \tTraining Loss: 0.000013 \tValidation Loss: 0.122411\n","Epoch: 684 \tTraining Loss: 0.000013 \tValidation Loss: 0.122979\n","Epoch: 685 \tTraining Loss: 0.000013 \tValidation Loss: 0.122605\n","Epoch: 686 \tTraining Loss: 0.000013 \tValidation Loss: 0.122890\n","Epoch: 687 \tTraining Loss: 0.000013 \tValidation Loss: 0.122927\n","Epoch: 688 \tTraining Loss: 0.000012 \tValidation Loss: 0.123342\n","Epoch: 689 \tTraining Loss: 0.000012 \tValidation Loss: 0.123466\n","Epoch: 690 \tTraining Loss: 0.000012 \tValidation Loss: 0.123005\n","Epoch: 691 \tTraining Loss: 0.000012 \tValidation Loss: 0.123166\n","Epoch: 692 \tTraining Loss: 0.000012 \tValidation Loss: 0.123253\n","Epoch: 693 \tTraining Loss: 0.000012 \tValidation Loss: 0.123867\n","Epoch: 694 \tTraining Loss: 0.000012 \tValidation Loss: 0.123211\n","Epoch: 695 \tTraining Loss: 0.000012 \tValidation Loss: 0.123582\n","Epoch: 696 \tTraining Loss: 0.000012 \tValidation Loss: 0.123858\n","Epoch: 697 \tTraining Loss: 0.000012 \tValidation Loss: 0.123770\n","Epoch: 698 \tTraining Loss: 0.000011 \tValidation Loss: 0.123812\n","Epoch: 699 \tTraining Loss: 0.000011 \tValidation Loss: 0.123976\n","Epoch: 700 \tTraining Loss: 0.000011 \tValidation Loss: 0.123847\n","Epoch: 701 \tTraining Loss: 0.000011 \tValidation Loss: 0.124515\n","Epoch: 702 \tTraining Loss: 0.000011 \tValidation Loss: 0.123927\n","Epoch: 703 \tTraining Loss: 0.000011 \tValidation Loss: 0.124454\n","Epoch: 704 \tTraining Loss: 0.000011 \tValidation Loss: 0.124539\n","Epoch: 705 \tTraining Loss: 0.000011 \tValidation Loss: 0.124271\n","Epoch: 706 \tTraining Loss: 0.000011 \tValidation Loss: 0.124539\n","Epoch: 707 \tTraining Loss: 0.000011 \tValidation Loss: 0.124793\n","Epoch: 708 \tTraining Loss: 0.000011 \tValidation Loss: 0.124706\n","Epoch: 709 \tTraining Loss: 0.000010 \tValidation Loss: 0.124732\n","Epoch: 710 \tTraining Loss: 0.000010 \tValidation Loss: 0.124600\n","Epoch: 711 \tTraining Loss: 0.000010 \tValidation Loss: 0.125087\n","Epoch: 712 \tTraining Loss: 0.000010 \tValidation Loss: 0.125145\n","Epoch: 713 \tTraining Loss: 0.000010 \tValidation Loss: 0.124906\n","Epoch: 714 \tTraining Loss: 0.000010 \tValidation Loss: 0.125500\n","Epoch: 715 \tTraining Loss: 0.000010 \tValidation Loss: 0.125142\n","Epoch: 716 \tTraining Loss: 0.000010 \tValidation Loss: 0.125069\n","Epoch: 717 \tTraining Loss: 0.000010 \tValidation Loss: 0.125465\n","Epoch: 718 \tTraining Loss: 0.000010 \tValidation Loss: 0.125637\n","Epoch: 719 \tTraining Loss: 0.000010 \tValidation Loss: 0.125662\n","Epoch: 720 \tTraining Loss: 0.000010 \tValidation Loss: 0.125785\n","Epoch: 721 \tTraining Loss: 0.000009 \tValidation Loss: 0.125382\n","Epoch: 722 \tTraining Loss: 0.000009 \tValidation Loss: 0.125929\n","Epoch: 723 \tTraining Loss: 0.000009 \tValidation Loss: 0.125911\n","Epoch: 724 \tTraining Loss: 0.000009 \tValidation Loss: 0.125994\n","Epoch: 725 \tTraining Loss: 0.000009 \tValidation Loss: 0.125767\n","Epoch: 726 \tTraining Loss: 0.000009 \tValidation Loss: 0.126247\n","Epoch: 727 \tTraining Loss: 0.000009 \tValidation Loss: 0.126275\n","Epoch: 728 \tTraining Loss: 0.000009 \tValidation Loss: 0.126172\n","Epoch: 729 \tTraining Loss: 0.000009 \tValidation Loss: 0.126333\n","Epoch: 730 \tTraining Loss: 0.000009 \tValidation Loss: 0.126392\n","Epoch: 731 \tTraining Loss: 0.000009 \tValidation Loss: 0.126506\n","Epoch: 732 \tTraining Loss: 0.000009 \tValidation Loss: 0.126627\n","Epoch: 733 \tTraining Loss: 0.000009 \tValidation Loss: 0.126335\n","Epoch: 734 \tTraining Loss: 0.000009 \tValidation Loss: 0.126714\n","Epoch: 735 \tTraining Loss: 0.000009 \tValidation Loss: 0.127053\n","Epoch: 736 \tTraining Loss: 0.000008 \tValidation Loss: 0.126645\n","Epoch: 737 \tTraining Loss: 0.000008 \tValidation Loss: 0.126951\n","Epoch: 738 \tTraining Loss: 0.000008 \tValidation Loss: 0.127128\n","Epoch: 739 \tTraining Loss: 0.000008 \tValidation Loss: 0.126962\n","Epoch: 740 \tTraining Loss: 0.000008 \tValidation Loss: 0.127482\n","Epoch: 741 \tTraining Loss: 0.000008 \tValidation Loss: 0.127014\n","Epoch: 742 \tTraining Loss: 0.000008 \tValidation Loss: 0.127148\n","Epoch: 743 \tTraining Loss: 0.000008 \tValidation Loss: 0.127584\n","Epoch: 744 \tTraining Loss: 0.000008 \tValidation Loss: 0.127570\n","Epoch: 745 \tTraining Loss: 0.000008 \tValidation Loss: 0.127340\n","Epoch: 746 \tTraining Loss: 0.000008 \tValidation Loss: 0.127653\n","Epoch: 747 \tTraining Loss: 0.000008 \tValidation Loss: 0.127668\n","Epoch: 748 \tTraining Loss: 0.000008 \tValidation Loss: 0.127734\n","Epoch: 749 \tTraining Loss: 0.000008 \tValidation Loss: 0.127857\n","Epoch: 750 \tTraining Loss: 0.000008 \tValidation Loss: 0.127886\n","Epoch: 751 \tTraining Loss: 0.000007 \tValidation Loss: 0.128129\n","Epoch: 752 \tTraining Loss: 0.000007 \tValidation Loss: 0.127677\n","Epoch: 753 \tTraining Loss: 0.000007 \tValidation Loss: 0.128245\n","Epoch: 754 \tTraining Loss: 0.000007 \tValidation Loss: 0.128436\n","Epoch: 755 \tTraining Loss: 0.000007 \tValidation Loss: 0.128290\n","Epoch: 756 \tTraining Loss: 0.000007 \tValidation Loss: 0.128178\n","Epoch: 757 \tTraining Loss: 0.000007 \tValidation Loss: 0.128471\n","Epoch: 758 \tTraining Loss: 0.000007 \tValidation Loss: 0.128694\n","Epoch: 759 \tTraining Loss: 0.000007 \tValidation Loss: 0.128485\n","Epoch: 760 \tTraining Loss: 0.000007 \tValidation Loss: 0.128648\n","Epoch: 761 \tTraining Loss: 0.000007 \tValidation Loss: 0.128679\n","Epoch: 762 \tTraining Loss: 0.000007 \tValidation Loss: 0.128858\n","Epoch: 763 \tTraining Loss: 0.000007 \tValidation Loss: 0.129077\n","Epoch: 764 \tTraining Loss: 0.000007 \tValidation Loss: 0.128983\n","Epoch: 765 \tTraining Loss: 0.000007 \tValidation Loss: 0.128992\n","Epoch: 766 \tTraining Loss: 0.000007 \tValidation Loss: 0.128992\n","Epoch: 767 \tTraining Loss: 0.000007 \tValidation Loss: 0.129483\n","Epoch: 768 \tTraining Loss: 0.000007 \tValidation Loss: 0.129249\n","Epoch: 769 \tTraining Loss: 0.000006 \tValidation Loss: 0.129258\n","Epoch: 770 \tTraining Loss: 0.000006 \tValidation Loss: 0.129454\n","Epoch: 771 \tTraining Loss: 0.000006 \tValidation Loss: 0.129574\n","Epoch: 772 \tTraining Loss: 0.000006 \tValidation Loss: 0.129588\n","Epoch: 773 \tTraining Loss: 0.000006 \tValidation Loss: 0.129559\n","Epoch: 774 \tTraining Loss: 0.000006 \tValidation Loss: 0.129780\n","Epoch: 775 \tTraining Loss: 0.000006 \tValidation Loss: 0.129610\n","Epoch: 776 \tTraining Loss: 0.000006 \tValidation Loss: 0.129788\n","Epoch: 777 \tTraining Loss: 0.000006 \tValidation Loss: 0.129833\n","Epoch: 778 \tTraining Loss: 0.000006 \tValidation Loss: 0.130059\n","Epoch: 779 \tTraining Loss: 0.000006 \tValidation Loss: 0.130120\n","Epoch: 780 \tTraining Loss: 0.000006 \tValidation Loss: 0.129890\n","Epoch: 781 \tTraining Loss: 0.000006 \tValidation Loss: 0.130313\n","Epoch: 782 \tTraining Loss: 0.000006 \tValidation Loss: 0.130410\n","Epoch: 783 \tTraining Loss: 0.000006 \tValidation Loss: 0.130284\n","Epoch: 784 \tTraining Loss: 0.000006 \tValidation Loss: 0.130392\n","Epoch: 785 \tTraining Loss: 0.000006 \tValidation Loss: 0.130726\n","Epoch: 786 \tTraining Loss: 0.000006 \tValidation Loss: 0.130747\n","Epoch: 787 \tTraining Loss: 0.000006 \tValidation Loss: 0.130532\n","Epoch: 788 \tTraining Loss: 0.000006 \tValidation Loss: 0.130642\n","Epoch: 789 \tTraining Loss: 0.000006 \tValidation Loss: 0.130856\n","Epoch: 790 \tTraining Loss: 0.000006 \tValidation Loss: 0.131052\n","Epoch: 791 \tTraining Loss: 0.000006 \tValidation Loss: 0.130730\n","Epoch: 792 \tTraining Loss: 0.000005 \tValidation Loss: 0.131071\n","Epoch: 793 \tTraining Loss: 0.000005 \tValidation Loss: 0.131199\n","Epoch: 794 \tTraining Loss: 0.000005 \tValidation Loss: 0.130854\n","Epoch: 795 \tTraining Loss: 0.000005 \tValidation Loss: 0.131306\n","Epoch: 796 \tTraining Loss: 0.000005 \tValidation Loss: 0.131291\n","Epoch: 797 \tTraining Loss: 0.000005 \tValidation Loss: 0.131505\n","Epoch: 798 \tTraining Loss: 0.000005 \tValidation Loss: 0.131367\n","Epoch: 799 \tTraining Loss: 0.000005 \tValidation Loss: 0.131379\n","Epoch: 800 \tTraining Loss: 0.000005 \tValidation Loss: 0.131743\n","Epoch: 801 \tTraining Loss: 0.000005 \tValidation Loss: 0.131519\n","Epoch: 802 \tTraining Loss: 0.000005 \tValidation Loss: 0.131970\n","Epoch: 803 \tTraining Loss: 0.000005 \tValidation Loss: 0.131848\n","Epoch: 804 \tTraining Loss: 0.000005 \tValidation Loss: 0.131622\n","Epoch: 805 \tTraining Loss: 0.000005 \tValidation Loss: 0.132003\n","Epoch: 806 \tTraining Loss: 0.000005 \tValidation Loss: 0.132052\n","Epoch: 807 \tTraining Loss: 0.000005 \tValidation Loss: 0.132373\n","Epoch: 808 \tTraining Loss: 0.000005 \tValidation Loss: 0.131567\n","Epoch: 809 \tTraining Loss: 0.000005 \tValidation Loss: 0.132149\n","Epoch: 810 \tTraining Loss: 0.000005 \tValidation Loss: 0.132584\n","Epoch: 811 \tTraining Loss: 0.000005 \tValidation Loss: 0.132208\n","Epoch: 812 \tTraining Loss: 0.000005 \tValidation Loss: 0.132673\n","Epoch: 813 \tTraining Loss: 0.000005 \tValidation Loss: 0.132419\n","Epoch: 814 \tTraining Loss: 0.000005 \tValidation Loss: 0.132578\n","Epoch: 815 \tTraining Loss: 0.000005 \tValidation Loss: 0.132552\n","Epoch: 816 \tTraining Loss: 0.000005 \tValidation Loss: 0.132793\n","Epoch: 817 \tTraining Loss: 0.000005 \tValidation Loss: 0.132709\n","Epoch: 818 \tTraining Loss: 0.000005 \tValidation Loss: 0.132985\n","Epoch: 819 \tTraining Loss: 0.000004 \tValidation Loss: 0.132989\n","Epoch: 820 \tTraining Loss: 0.000004 \tValidation Loss: 0.132875\n","Epoch: 821 \tTraining Loss: 0.000004 \tValidation Loss: 0.133037\n","Epoch: 822 \tTraining Loss: 0.000004 \tValidation Loss: 0.133147\n","Epoch: 823 \tTraining Loss: 0.000004 \tValidation Loss: 0.133204\n","Epoch: 824 \tTraining Loss: 0.000004 \tValidation Loss: 0.133327\n","Epoch: 825 \tTraining Loss: 0.000004 \tValidation Loss: 0.133455\n","Epoch: 826 \tTraining Loss: 0.000004 \tValidation Loss: 0.133482\n","Epoch: 827 \tTraining Loss: 0.000004 \tValidation Loss: 0.133386\n","Epoch: 828 \tTraining Loss: 0.000004 \tValidation Loss: 0.133433\n","Epoch: 829 \tTraining Loss: 0.000004 \tValidation Loss: 0.133532\n","Epoch: 830 \tTraining Loss: 0.000004 \tValidation Loss: 0.133866\n","Epoch: 831 \tTraining Loss: 0.000004 \tValidation Loss: 0.133899\n","Epoch: 832 \tTraining Loss: 0.000004 \tValidation Loss: 0.133596\n","Epoch: 833 \tTraining Loss: 0.000004 \tValidation Loss: 0.133935\n","Epoch: 834 \tTraining Loss: 0.000004 \tValidation Loss: 0.134146\n","Epoch: 835 \tTraining Loss: 0.000004 \tValidation Loss: 0.134009\n","Epoch: 836 \tTraining Loss: 0.000004 \tValidation Loss: 0.134053\n","Epoch: 837 \tTraining Loss: 0.000004 \tValidation Loss: 0.134222\n","Epoch: 838 \tTraining Loss: 0.000004 \tValidation Loss: 0.134161\n","Epoch: 839 \tTraining Loss: 0.000004 \tValidation Loss: 0.134334\n","Epoch: 840 \tTraining Loss: 0.000004 \tValidation Loss: 0.134498\n","Epoch: 841 \tTraining Loss: 0.000004 \tValidation Loss: 0.134504\n","Epoch: 842 \tTraining Loss: 0.000004 \tValidation Loss: 0.134577\n","Epoch: 843 \tTraining Loss: 0.000004 \tValidation Loss: 0.134409\n","Epoch: 844 \tTraining Loss: 0.000004 \tValidation Loss: 0.134622\n","Epoch: 845 \tTraining Loss: 0.000004 \tValidation Loss: 0.134875\n","Epoch: 846 \tTraining Loss: 0.000004 \tValidation Loss: 0.134715\n","Epoch: 847 \tTraining Loss: 0.000004 \tValidation Loss: 0.134743\n","Epoch: 848 \tTraining Loss: 0.000004 \tValidation Loss: 0.134961\n","Epoch: 849 \tTraining Loss: 0.000004 \tValidation Loss: 0.135021\n","Epoch: 850 \tTraining Loss: 0.000004 \tValidation Loss: 0.135197\n","Epoch: 851 \tTraining Loss: 0.000004 \tValidation Loss: 0.135227\n","Epoch: 852 \tTraining Loss: 0.000004 \tValidation Loss: 0.134962\n","Epoch: 853 \tTraining Loss: 0.000003 \tValidation Loss: 0.135524\n","Epoch: 854 \tTraining Loss: 0.000003 \tValidation Loss: 0.135345\n","Epoch: 855 \tTraining Loss: 0.000003 \tValidation Loss: 0.135315\n","Epoch: 856 \tTraining Loss: 0.000003 \tValidation Loss: 0.135470\n","Epoch: 857 \tTraining Loss: 0.000003 \tValidation Loss: 0.135545\n","Epoch: 858 \tTraining Loss: 0.000003 \tValidation Loss: 0.135795\n","Epoch: 859 \tTraining Loss: 0.000003 \tValidation Loss: 0.135635\n","Epoch: 860 \tTraining Loss: 0.000003 \tValidation Loss: 0.135660\n","Epoch: 861 \tTraining Loss: 0.000003 \tValidation Loss: 0.135835\n","Epoch: 862 \tTraining Loss: 0.000003 \tValidation Loss: 0.136026\n","Epoch: 863 \tTraining Loss: 0.000003 \tValidation Loss: 0.135915\n","Epoch: 864 \tTraining Loss: 0.000003 \tValidation Loss: 0.136082\n","Epoch: 865 \tTraining Loss: 0.000003 \tValidation Loss: 0.136226\n","Epoch: 866 \tTraining Loss: 0.000003 \tValidation Loss: 0.136233\n","Epoch: 867 \tTraining Loss: 0.000003 \tValidation Loss: 0.136067\n","Epoch: 868 \tTraining Loss: 0.000003 \tValidation Loss: 0.136191\n","Epoch: 869 \tTraining Loss: 0.000003 \tValidation Loss: 0.136709\n","Epoch: 870 \tTraining Loss: 0.000003 \tValidation Loss: 0.136554\n","Epoch: 871 \tTraining Loss: 0.000003 \tValidation Loss: 0.136285\n","Epoch: 872 \tTraining Loss: 0.000003 \tValidation Loss: 0.136486\n","Epoch: 873 \tTraining Loss: 0.000003 \tValidation Loss: 0.136688\n","Epoch: 874 \tTraining Loss: 0.000003 \tValidation Loss: 0.136930\n","Epoch: 875 \tTraining Loss: 0.000003 \tValidation Loss: 0.136491\n","Epoch: 876 \tTraining Loss: 0.000003 \tValidation Loss: 0.136984\n","Epoch: 877 \tTraining Loss: 0.000003 \tValidation Loss: 0.136941\n","Epoch: 878 \tTraining Loss: 0.000003 \tValidation Loss: 0.136818\n","Epoch: 879 \tTraining Loss: 0.000003 \tValidation Loss: 0.137151\n","Epoch: 880 \tTraining Loss: 0.000003 \tValidation Loss: 0.137247\n","Epoch: 881 \tTraining Loss: 0.000003 \tValidation Loss: 0.137114\n","Epoch: 882 \tTraining Loss: 0.000003 \tValidation Loss: 0.137146\n","Epoch: 883 \tTraining Loss: 0.000003 \tValidation Loss: 0.137526\n","Epoch: 884 \tTraining Loss: 0.000003 \tValidation Loss: 0.137391\n","Epoch: 885 \tTraining Loss: 0.000003 \tValidation Loss: 0.137401\n","Epoch: 886 \tTraining Loss: 0.000003 \tValidation Loss: 0.137229\n","Epoch: 887 \tTraining Loss: 0.000003 \tValidation Loss: 0.137764\n","Epoch: 888 \tTraining Loss: 0.000003 \tValidation Loss: 0.137716\n","Epoch: 889 \tTraining Loss: 0.000003 \tValidation Loss: 0.137952\n","Epoch: 890 \tTraining Loss: 0.000003 \tValidation Loss: 0.137617\n","Epoch: 891 \tTraining Loss: 0.000003 \tValidation Loss: 0.137688\n","Epoch: 892 \tTraining Loss: 0.000003 \tValidation Loss: 0.137999\n","Epoch: 893 \tTraining Loss: 0.000003 \tValidation Loss: 0.138272\n","Epoch: 894 \tTraining Loss: 0.000003 \tValidation Loss: 0.137960\n","Epoch: 895 \tTraining Loss: 0.000003 \tValidation Loss: 0.137808\n","Epoch: 896 \tTraining Loss: 0.000003 \tValidation Loss: 0.138307\n","Epoch: 897 \tTraining Loss: 0.000003 \tValidation Loss: 0.138203\n","Epoch: 898 \tTraining Loss: 0.000003 \tValidation Loss: 0.138405\n","Epoch: 899 \tTraining Loss: 0.000003 \tValidation Loss: 0.138455\n","Epoch: 900 \tTraining Loss: 0.000003 \tValidation Loss: 0.138107\n","Epoch: 901 \tTraining Loss: 0.000002 \tValidation Loss: 0.138511\n","Epoch: 902 \tTraining Loss: 0.000002 \tValidation Loss: 0.138738\n","Epoch: 903 \tTraining Loss: 0.000002 \tValidation Loss: 0.138669\n","Epoch: 904 \tTraining Loss: 0.000002 \tValidation Loss: 0.138530\n","Epoch: 905 \tTraining Loss: 0.000002 \tValidation Loss: 0.138724\n","Epoch: 906 \tTraining Loss: 0.000002 \tValidation Loss: 0.138881\n","Epoch: 907 \tTraining Loss: 0.000002 \tValidation Loss: 0.138987\n","Epoch: 908 \tTraining Loss: 0.000002 \tValidation Loss: 0.139099\n","Epoch: 909 \tTraining Loss: 0.000002 \tValidation Loss: 0.138840\n","Epoch: 910 \tTraining Loss: 0.000002 \tValidation Loss: 0.139107\n","Epoch: 911 \tTraining Loss: 0.000002 \tValidation Loss: 0.139084\n","Epoch: 912 \tTraining Loss: 0.000002 \tValidation Loss: 0.139482\n","Epoch: 913 \tTraining Loss: 0.000002 \tValidation Loss: 0.139190\n","Epoch: 914 \tTraining Loss: 0.000002 \tValidation Loss: 0.139472\n","Epoch: 915 \tTraining Loss: 0.000002 \tValidation Loss: 0.139300\n","Epoch: 916 \tTraining Loss: 0.000002 \tValidation Loss: 0.139498\n","Epoch: 917 \tTraining Loss: 0.000002 \tValidation Loss: 0.139451\n","Epoch: 918 \tTraining Loss: 0.000002 \tValidation Loss: 0.139420\n","Epoch: 919 \tTraining Loss: 0.000002 \tValidation Loss: 0.139822\n","Epoch: 920 \tTraining Loss: 0.000002 \tValidation Loss: 0.139769\n","Epoch: 921 \tTraining Loss: 0.000002 \tValidation Loss: 0.139892\n","Epoch: 922 \tTraining Loss: 0.000002 \tValidation Loss: 0.139784\n","Epoch: 923 \tTraining Loss: 0.000002 \tValidation Loss: 0.139942\n","Epoch: 924 \tTraining Loss: 0.000002 \tValidation Loss: 0.140113\n","Epoch: 925 \tTraining Loss: 0.000002 \tValidation Loss: 0.139967\n","Epoch: 926 \tTraining Loss: 0.000002 \tValidation Loss: 0.139990\n","Epoch: 927 \tTraining Loss: 0.000002 \tValidation Loss: 0.140474\n","Epoch: 928 \tTraining Loss: 0.000002 \tValidation Loss: 0.140095\n","Epoch: 929 \tTraining Loss: 0.000002 \tValidation Loss: 0.140306\n","Epoch: 930 \tTraining Loss: 0.000002 \tValidation Loss: 0.140634\n","Epoch: 931 \tTraining Loss: 0.000002 \tValidation Loss: 0.140375\n","Epoch: 932 \tTraining Loss: 0.000002 \tValidation Loss: 0.140484\n","Epoch: 933 \tTraining Loss: 0.000002 \tValidation Loss: 0.140651\n","Epoch: 934 \tTraining Loss: 0.000002 \tValidation Loss: 0.140645\n","Epoch: 935 \tTraining Loss: 0.000002 \tValidation Loss: 0.140533\n","Epoch: 936 \tTraining Loss: 0.000002 \tValidation Loss: 0.140945\n","Epoch: 937 \tTraining Loss: 0.000002 \tValidation Loss: 0.140810\n","Epoch: 938 \tTraining Loss: 0.000002 \tValidation Loss: 0.140785\n","Epoch: 939 \tTraining Loss: 0.000002 \tValidation Loss: 0.141108\n","Epoch: 940 \tTraining Loss: 0.000002 \tValidation Loss: 0.141188\n","Epoch: 941 \tTraining Loss: 0.000002 \tValidation Loss: 0.141123\n","Epoch: 942 \tTraining Loss: 0.000002 \tValidation Loss: 0.141090\n","Epoch: 943 \tTraining Loss: 0.000002 \tValidation Loss: 0.141352\n","Epoch: 944 \tTraining Loss: 0.000002 \tValidation Loss: 0.141248\n","Epoch: 945 \tTraining Loss: 0.000002 \tValidation Loss: 0.141373\n","Epoch: 946 \tTraining Loss: 0.000002 \tValidation Loss: 0.141418\n","Epoch: 947 \tTraining Loss: 0.000002 \tValidation Loss: 0.141354\n","Epoch: 948 \tTraining Loss: 0.000002 \tValidation Loss: 0.141881\n","Epoch: 949 \tTraining Loss: 0.000002 \tValidation Loss: 0.141645\n","Epoch: 950 \tTraining Loss: 0.000002 \tValidation Loss: 0.141493\n","Epoch: 951 \tTraining Loss: 0.000002 \tValidation Loss: 0.141775\n","Epoch: 952 \tTraining Loss: 0.000002 \tValidation Loss: 0.141775\n","Epoch: 953 \tTraining Loss: 0.000002 \tValidation Loss: 0.142258\n","Epoch: 954 \tTraining Loss: 0.000002 \tValidation Loss: 0.141974\n","Epoch: 955 \tTraining Loss: 0.000002 \tValidation Loss: 0.141903\n","Epoch: 956 \tTraining Loss: 0.000002 \tValidation Loss: 0.141972\n","Epoch: 957 \tTraining Loss: 0.000002 \tValidation Loss: 0.142150\n","Epoch: 958 \tTraining Loss: 0.000002 \tValidation Loss: 0.142306\n","Epoch: 959 \tTraining Loss: 0.000002 \tValidation Loss: 0.142443\n","Epoch: 960 \tTraining Loss: 0.000002 \tValidation Loss: 0.142030\n","Epoch: 961 \tTraining Loss: 0.000002 \tValidation Loss: 0.142502\n","Epoch: 962 \tTraining Loss: 0.000002 \tValidation Loss: 0.142439\n","Epoch: 963 \tTraining Loss: 0.000002 \tValidation Loss: 0.142646\n","Epoch: 964 \tTraining Loss: 0.000002 \tValidation Loss: 0.142631\n","Epoch: 965 \tTraining Loss: 0.000002 \tValidation Loss: 0.142597\n","Epoch: 966 \tTraining Loss: 0.000002 \tValidation Loss: 0.142802\n","Epoch: 967 \tTraining Loss: 0.000002 \tValidation Loss: 0.142716\n","Epoch: 968 \tTraining Loss: 0.000002 \tValidation Loss: 0.143005\n","Epoch: 969 \tTraining Loss: 0.000002 \tValidation Loss: 0.142870\n","Epoch: 970 \tTraining Loss: 0.000002 \tValidation Loss: 0.142698\n","Epoch: 971 \tTraining Loss: 0.000002 \tValidation Loss: 0.143237\n","Epoch: 972 \tTraining Loss: 0.000001 \tValidation Loss: 0.143160\n","Epoch: 973 \tTraining Loss: 0.000001 \tValidation Loss: 0.143106\n","Epoch: 974 \tTraining Loss: 0.000001 \tValidation Loss: 0.143479\n","Epoch: 975 \tTraining Loss: 0.000001 \tValidation Loss: 0.143222\n","Epoch: 976 \tTraining Loss: 0.000001 \tValidation Loss: 0.143416\n","Epoch: 977 \tTraining Loss: 0.000001 \tValidation Loss: 0.143388\n","Epoch: 978 \tTraining Loss: 0.000001 \tValidation Loss: 0.143605\n","Epoch: 979 \tTraining Loss: 0.000001 \tValidation Loss: 0.143546\n","Epoch: 980 \tTraining Loss: 0.000001 \tValidation Loss: 0.143678\n","Epoch: 981 \tTraining Loss: 0.000001 \tValidation Loss: 0.143695\n","Epoch: 982 \tTraining Loss: 0.000001 \tValidation Loss: 0.143702\n","Epoch: 983 \tTraining Loss: 0.000001 \tValidation Loss: 0.143877\n","Epoch: 984 \tTraining Loss: 0.000001 \tValidation Loss: 0.143872\n","Epoch: 985 \tTraining Loss: 0.000001 \tValidation Loss: 0.143945\n","Epoch: 986 \tTraining Loss: 0.000001 \tValidation Loss: 0.144084\n","Epoch: 987 \tTraining Loss: 0.000001 \tValidation Loss: 0.144028\n","Epoch: 988 \tTraining Loss: 0.000001 \tValidation Loss: 0.144415\n","Epoch: 989 \tTraining Loss: 0.000001 \tValidation Loss: 0.144074\n","Epoch: 990 \tTraining Loss: 0.000001 \tValidation Loss: 0.144165\n","Epoch: 991 \tTraining Loss: 0.000001 \tValidation Loss: 0.144522\n","Epoch: 992 \tTraining Loss: 0.000001 \tValidation Loss: 0.144347\n","Epoch: 993 \tTraining Loss: 0.000001 \tValidation Loss: 0.144484\n","Epoch: 994 \tTraining Loss: 0.000001 \tValidation Loss: 0.144405\n","Epoch: 995 \tTraining Loss: 0.000001 \tValidation Loss: 0.144543\n","Epoch: 996 \tTraining Loss: 0.000001 \tValidation Loss: 0.144716\n","Epoch: 997 \tTraining Loss: 0.000001 \tValidation Loss: 0.144776\n","Epoch: 998 \tTraining Loss: 0.000001 \tValidation Loss: 0.144747\n","Epoch: 999 \tTraining Loss: 0.000001 \tValidation Loss: 0.144944\n","Epoch: 1000 \tTraining Loss: 0.000001 \tValidation Loss: 0.144894\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d9rcfRRO8jUU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"425b9e08-21c4-4197-dba6-1c98925f275b","executionInfo":{"status":"ok","timestamp":1560230421224,"user_tz":-330,"elapsed":993,"user":{"displayName":"ANUSHA SHENOY (01FB15EEC044)PESU ECE STUDENT","photoUrl":"","userId":"17943164030553000523"}}},"source":["# track test loss\n","test_loss = 0.0\n","class_correct = list(0. for i in range(2))\n","class_total = list(0. for i in range(2))\n","\n","batch_size=64\n","\n","model.eval()\n","i=1\n","# iterate over test data\n","len(testloader)\n","for data, target in testloader:\n","    i=i+1\n","    \n","\n","    \n","    if len(target)!=batch_size:\n","        continue\n","        \n","    # move tensors to GPU if CUDA is available\n","    if train_on_gpu:\n","        data, target = data.cuda(), target.cuda()\n","    # forward pass: compute predicted outputs by passing inputs to the model\n","    output = model(data)\n","    # calculate the batch loss\n","    loss = criterion(output, target)\n","    # update test loss \n","    test_loss += loss.item()*data.size(0)\n","    # convert output probabilities to predicted class\n","    _, pred = torch.max(output, 1)    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(target.data.view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    # calculate test accuracy for each object class\n","#     print(target)\n","    \n","    for i in range(batch_size):       \n","        label = target.data[i]\n","        class_correct[label] += correct[i].item()\n","        class_total[label] += 1\n","\n","# average test loss\n","test_loss = test_loss/len(testloader.dataset)\n","print('Test Loss: {:.6f}\\n'.format(test_loss))\n","\n","print('\\n ')\n","print(n_epochs)\n","for i in range(2):\n","    if class_total[i] > 0:\n","        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n","            classes[i], 100 * class_correct[i] / class_total[i],\n","            np.sum(class_correct[i]), np.sum(class_total[i])))\n","    else:\n","        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n","\n","print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","    100. * np.sum(class_correct) / np.sum(class_total),\n","    np.sum(class_correct), np.sum(class_total)))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Test Loss: 1.129172\n","\n","\n"," \n","1000\n","Test Accuracy of     0: 34% (23/66)\n","Test Accuracy of     1: 53% (33/62)\n","\n","Test Accuracy (Overall): 43% (56/128)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RAD2l-MK8qRf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}